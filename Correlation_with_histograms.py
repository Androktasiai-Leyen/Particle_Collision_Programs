#!/usr/bin/env python3
"""
å¢å¼ºçš„ä¸¤ç²’å­å…³è”åˆ†æç¨‹åº - åŒ…å«ä¿¡å·å’ŒèƒŒæ™¯ç›´æ–¹å›¾ç»˜åˆ¶
é’ˆå¯¹ multiplicity_group_0-20_percent.txt æ•°æ®æ ¼å¼
pTèŒƒå›´ï¼š0.5-5.0 GeVï¼Œ|Î·| âˆˆ [-1.1, 1.1]
æ”¯æŒROOTæ–‡ä»¶è¾“å‡ºï¼Œä¾¿äºè¿›ä¸€æ­¥åˆ†æ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import cm
from tqdm.auto import tqdm
import os
import gc
import time
import argparse
import numba
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# å¯ç”¨å†…å­˜ç›‘æ§
try:
    import psutil
    MEMORY_MONITOR = True
except ImportError:
    MEMORY_MONITOR = False

# ROOTç›¸å…³å¯¼å…¥ - ä¿®å¤å¯¼å…¥é—®é¢˜
ROOT_AVAILABLE = False
try:
    import ROOT
    ROOT_AVAILABLE = True
    print("âœ… ROOTåº“å¯ç”¨ï¼Œå°†ç”ŸæˆROOTæ–‡ä»¶")
except ImportError:
    print("âš ï¸ ROOTåº“ä¸å¯ç”¨ï¼Œå°†è·³è¿‡ROOTæ–‡ä»¶ç”Ÿæˆ")
    print("   è¯·å®‰è£…ROOT: https://root.cern/install/")
    print("   æˆ–è€…ä½¿ç”¨conda: conda install -c conda-forge root")
except Exception as e:
    print(f"âš ï¸ ROOTåº“å¯¼å…¥å‡ºé”™: {e}")
    print("   å°†è·³è¿‡ROOTæ–‡ä»¶ç”Ÿæˆ")

@numba.njit
def calculate_delta_phi(phi1, phi2):
    """å¿«é€Ÿè®¡ç®—è§’åº¦å·®å¹¶å½’ä¸€åŒ–åˆ°[-Ï€/2, 3Ï€/2]èŒƒå›´"""
    delta = phi1 - phi2
    delta = (delta + np.pi) % (2 * np.pi) - np.pi
    if delta < -np.pi/2:
        delta += 2 * np.pi
    return delta

@numba.njit(parallel=True)
def vectorized_delta_calculation(eta1, phi1, eta2, phi2):
    """å‘é‡åŒ–è®¡ç®—etaå’Œphiå·®å¼‚"""
    n = len(eta1)
    delta_eta = np.empty(n, dtype=np.float32)
    delta_phi = np.empty(n, dtype=np.float32)
    
    for i in numba.prange(n):
        delta_eta[i] = eta1[i] - eta2[i]
        delta_phi[i] = calculate_delta_phi(phi1[i], phi2[i])
    
    return delta_eta, delta_phi

def load_data_from_txt(data_file, pt_min=0.5, pt_max=5.0, eta_min=2.0, eta_max=5.0):
    """
    ä»txtæ–‡ä»¶åŠ è½½æ•°æ®ï¼Œæ ¼å¼ï¼ševent_id particle_id particle_type pt phi eta
    """
    print(f"ğŸ“‚ Loading data from: {os.path.basename(data_file)}")
    print(f"ğŸ¯ pT range: [{pt_min}, {pt_max}] GeV")
    print(f"ğŸ¯ |Î·| range: [{eta_min}, {eta_max}]")
    
    # å­˜å‚¨äº‹ä»¶æ•°æ®
    event_data = {}
    total_particles = 0
    filtered_particles = 0
    
    # è·å–æ–‡ä»¶å¤§å°
    file_size = os.path.getsize(data_file)
    file_size_mb = file_size / (1024**2)
    print(f"ğŸ“ File size: {file_size_mb:.2f} MB")
    
    # è¯»å–æ•°æ®
    with open(data_file, 'r') as f:
        for line_num, line in enumerate(f):
            if not line.strip():
                continue
                
            parts = line.strip().split()
            if len(parts) != 6:
                continue
            
            try:
                event_id = parts[0]  # ä¿æŒä¸ºå­—ç¬¦ä¸²
                particle_id = int(parts[1])
                particle_type = int(parts[2])
                pt = float(parts[3])
                phi = float(parts[4])
                eta = float(parts[5])
                
                total_particles += 1
                
                # åº”ç”¨ç­›é€‰æ¡ä»¶
                if (pt_min <= pt <= pt_max and eta_min <= abs(eta) <= eta_max):
                    filtered_particles += 1
                    
                    if event_id not in event_data:
                        event_data[event_id] = []
                    
                    event_data[event_id].append([eta, phi, pt, particle_type])
                
            except (ValueError, IndexError):
                continue
            
            # æ˜¾ç¤ºè¿›åº¦
            if (line_num + 1) % 100000 == 0:
                print(f"ğŸ“Š Processed {line_num + 1:,} lines, found {len(event_data):,} events")
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    for event_id in event_data:
        event_data[event_id] = np.array(event_data[event_id], dtype=np.float32)
    
    print(f"\nâœ… Data loading completed:")
    print(f"ğŸ“ˆ Total particles: {total_particles:,}")
    print(f"ğŸ“Š Filtered particles: {filtered_particles:,}")
    print(f"ğŸ“Š Events with valid particles: {len(event_data):,}")
    
    return event_data

def load_data_from_pbpb_csv(data_file, pt_min=0.5, pt_max=5.0, eta_min=-1.1, eta_max=1.1):
    """
    ä»é€—å·åˆ†éš”çš„ PbPb_20k.txt åŠ è½½æ•°æ®ï¼Œå­—æ®µä¸ºï¼ševent,track,pt,eta,phi,charge
    æ˜ å°„å…³ç³»ï¼š
      event_id=event (str)
      particle_id=track (int)
      pt=pt (float)
      eta=eta (float)
      phi=phi (float)
      particle_type â† charge (intï¼Œå ä½ï¼Œä¸åœ¨åç»­è®¡ç®—ä¸­ä½¿ç”¨)
    """
    print(f"ğŸ“‚ Loading CSV data from: {os.path.basename(data_file)}")
    print(f"ğŸ¯ pT range: [{pt_min}, {pt_max}] GeV")
    print(f"ğŸ¯ |Î·| range: [{eta_min}, {eta_max}]")

    event_data: dict[str, list] = {}
    total_particles = 0
    filtered_particles = 0

    file_size = os.path.getsize(data_file)
    print(f"ğŸ“ File size: {file_size/(1024**2):.2f} MB")

    with open(data_file, 'r') as f:
        for line_num, line in enumerate(f):
            line = line.strip()
            if not line:
                continue

            # å¯èƒ½å­˜åœ¨è¡¨å¤´ï¼Œå°è¯•è·³è¿‡éæ•°å­—èµ·å§‹è¡Œ
            if line_num == 0 and (',' in line):
                parts0 = [p.strip() for p in line.split(',')]
                # å¦‚æœç¬¬ä¸€åˆ—ä¸æ˜¯æ•°å­—ï¼ˆå¯èƒ½æ˜¯è¡¨å¤´ï¼‰ï¼Œåˆ™è·³è¿‡
                try:
                    float(parts0[0])
                except Exception:
                    # è·³è¿‡è¡¨å¤´è¡Œ
                    continue

            parts = [p.strip() for p in line.split(',')]
            if len(parts) < 6:
                continue

            try:
                event_id = parts[0]  # ä½œä¸ºå­—ç¬¦ä¸²é”®
                track_id = int(float(parts[1]))
                pt = float(parts[2])
                eta = float(parts[3])
                phi = float(parts[4])
                charge = int(float(parts[5])) if parts[5] else 0

                total_particles += 1

                # é€‰æ‹©æ¡ä»¶ï¼špt ä¸ |Î·|
                if (pt_min <= pt <= pt_max) and (eta_min <= eta <= eta_max):
                    filtered_particles += 1
                    if event_id not in event_data:
                        event_data[event_id] = []
                    # å­˜å‚¨é¡ºåºä¸ä¸‹æ¸¸ä¸€è‡´ï¼š[eta, phi, pt, particle_type]
                    event_data[event_id].append([eta, phi, pt, charge])
            except Exception:
                continue

            if (line_num + 1) % 100000 == 0:
                print(f"ğŸ“Š Processed {line_num + 1:,} lines, found {len(event_data):,} events")

    # è½¬ä¸º numpy æ•°ç»„
    for event_id in list(event_data.keys()):
        event_data[event_id] = np.array(event_data[event_id], dtype=np.float32)

    print(f"\nâœ… CSV loading completed:")
    print(f"ğŸ“ˆ Total particles: {total_particles:,}")
    print(f"ğŸ“Š Filtered particles: {filtered_particles:,}")
    print(f"ğŸ“Š Events with valid particles: {len(event_data):,}")

    return event_data

def analyze_multiplicity_distribution(event_data):
    """
    åˆ†æå¤šé‡åº¦åˆ†å¸ƒï¼Œç”¨äºéªŒè¯äº‹ä»¶æ··åˆ
    """
    print(f"\nğŸ“Š Analyzing multiplicity distribution...")
    
    multiplicities = [len(particles) for particles in event_data.values()]
    multiplicities = np.array(multiplicities)
    
    print(f"ğŸ“ˆ Multiplicity statistics:")
    print(f"   Mean: {np.mean(multiplicities):.2f}")
    print(f"   Std: {np.std(multiplicities):.2f}")
    print(f"   Min: {np.min(multiplicities)}")
    print(f"   Max: {np.max(multiplicities)}")
    print(f"   Events with mult >= 3: {np.sum(multiplicities >= 3):,}")
    print(f"   Events with mult >= 5: {np.sum(multiplicities >= 5):,}")
    
    # ç»˜åˆ¶å¤šé‡åº¦åˆ†å¸ƒ
    plt.figure(figsize=(10, 6))
    plt.hist(multiplicities, bins=50, alpha=0.7, edgecolor='black')
    plt.xlabel('Event Multiplicity')
    plt.ylabel('Number of Events')
    plt.title('Event Multiplicity Distribution')
    plt.yscale('log')
    plt.grid(True, alpha=0.3)
    plt.savefig('multiplicity_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ–¼ï¸ Multiplicity distribution saved to: multiplicity_distribution.png")
    
    return multiplicities

def apply_zyam_method(C, eta_bins, phi_bins):
    """
    åº”ç”¨ZYAMæ–¹æ³•ï¼šæ‰¾åˆ°æœ€å°å€¼å¹¶å½’é›¶
    ZYAM = Zero Yield At Minimum
    """
    print(f"\nğŸ” Applying ZYAM method...")
    
    C_zyam = C.copy()
    zyam_applied = 0
    
    # å¯¹æ¯ä¸ªÎ”Î·åˆ‡ç‰‡åº”ç”¨ZYAM
    for i in range(eta_bins):
        phi_slice = C[i, :]
        min_val = np.min(phi_slice)
        
        if min_val < 0:
            C_zyam[i, :] -= min_val
            zyam_applied += 1
            print(f"   Î”Î· bin {i}: min = {min_val:.4f}, applied ZYAM")
    
    # è®¡ç®—æ•´ä½“ç»Ÿè®¡
    overall_min = np.min(C)
    overall_max = np.max(C)
    zyam_min = np.min(C_zyam)
    zyam_max = np.max(C_zyam)
    
    print(f"ğŸ“Š ZYAM statistics:")
    print(f"   Original range: [{overall_min:.4f}, {overall_max:.4f}]")
    print(f"   ZYAM range: [{zyam_min:.4f}, {zyam_max:.4f}]")
    print(f"   ZYAM applied to {zyam_applied}/{eta_bins} Î”Î· bins")
    
    return C_zyam

def calculate_correlation_with_histograms(event_data, eta_bins=22, phi_bins=22, max_pairs=50000000):
    """
    è®¡ç®—å…³è”å‡½æ•°å¹¶è¿”å›ä¿¡å·å’ŒèƒŒæ™¯ç›´æ–¹å›¾
    """
    print(f"\nğŸ“Š Calculating correlation function...")
    print(f"ğŸ”¢ Grid: {eta_bins} Ã— {phi_bins}")
    print(f"ğŸ”¢ Max pairs: {max_pairs:,}")
    
    # å®šä¹‰èŒƒå›´
    eta_range = (-2.2, 2.2)  # ä¿®æ”¹ï¼šÎ”Î· èŒƒå›´è®¾ç½®ä¸º[-2.2,2.2]
    phi_range = (-np.pi/2, 3*np.pi/2)
    
    # åˆå§‹åŒ–ç›´æ–¹å›¾
    S_N = np.zeros((eta_bins, phi_bins), dtype=np.float64)
    B_N = np.zeros((eta_bins, phi_bins), dtype=np.float64)
    
    if not event_data:
        print("âš ï¸ No event data found!")
        return None, None, None, None, None
    
    n_events = len(event_data)
    event_list = list(event_data.values())
    
    print(f"ğŸ¯ Total events: {n_events:,}")
    
    # ä¿¡å·åˆ†å¸ƒ - å‘é‡åŒ–è®¡ç®—
    print("ğŸ” Computing signal distribution...")
    signal_pairs = 0
    np.random.seed(42)
    
    # é¢„è®¡ç®—äº‹ä»¶æƒé‡
    event_weights = np.array([len(particles) for particles in event_list], dtype=np.float64)
    event_probs = event_weights / event_weights.sum()
    
    # é‡‡æ ·äº‹ä»¶
    sampled_events = np.random.choice(n_events, size=max_pairs, p=event_probs)
    event_counts = np.bincount(sampled_events, minlength=n_events)
    
    with tqdm(total=len(event_counts), desc="Signal pairs", unit='events',
              bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as pbar:
        
        for i, count in enumerate(event_counts):
            if count == 0:
                pbar.update(1)
                continue
                
            particles = event_list[i]
            n_particles = len(particles)
            
            if n_particles < 2:
                pbar.update(1)
                continue
                
            # è®¡ç®—è¯¥äº‹ä»¶ä¸­éœ€è¦é‡‡æ ·çš„å¯¹æ•°
            n_pairs = min(count, n_particles * (n_particles - 1) // 2)
            if n_pairs == 0:
                pbar.update(1)
                continue
                
            # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„ç²’å­å¯¹ç´¢å¼•
            indices = np.triu_indices(n_particles, k=1)
            if n_pairs < len(indices[0]):
                selected = np.random.choice(len(indices[0]), size=n_pairs, replace=False)
                i_indices = indices[0][selected]
                j_indices = indices[1][selected]
            else:
                i_indices = indices[0]
                j_indices = indices[1]
            
            # æå–ç²’å­åæ ‡
            particles_arr = particles
            eta_i = particles_arr[i_indices, 0]
            phi_i = particles_arr[i_indices, 1]
            eta_j = particles_arr[j_indices, 0]
            phi_j = particles_arr[j_indices, 1]
            
            # è®¡ç®—å·®å¼‚
            delta_eta = eta_i - eta_j
            delta_phi = phi_i - phi_j
            
            # å‘é‡åŒ–phiå½’ä¸€åŒ–
            delta_phi = (delta_phi + np.pi) % (2 * np.pi) - np.pi
            delta_phi = np.where(delta_phi < -np.pi/2, delta_phi + 2*np.pi, delta_phi)
            
            # å‘é‡åŒ–ç›´æ–¹å›¾
            valid_mask = (
                (delta_eta >= eta_range[0]) & 
                (delta_eta <= eta_range[1]) & 
                (delta_phi >= phi_range[0]) & 
                (delta_phi <= phi_range[1])
            )
            
            if np.any(valid_mask):
                hist, _, _ = np.histogram2d(
                    delta_eta[valid_mask], delta_phi[valid_mask],
                    bins=[eta_bins, phi_bins], range=[eta_range, phi_range]
                )
                S_N += hist
                signal_pairs += np.sum(valid_mask)
            
            pbar.update(1)
    
    # èƒŒæ™¯åˆ†å¸ƒ - å‘é‡åŒ–æ··åˆäº‹ä»¶
    print("ğŸ” Computing background distribution...")
    background_pairs = 0

    # é‡‡æ ·äº‹ä»¶å¯¹ï¼Œä¿è¯ä¸åŒäº‹ä»¶
    np.random.seed(42)
    event_indices1 = np.random.randint(0, n_events, size=max_pairs)
    event_indices2 = np.random.randint(0, n_events, size=max_pairs)
    # ä¿è¯ event_indices1 != event_indices2
    mask = event_indices1 != event_indices2
    event_indices1 = event_indices1[mask]
    event_indices2 = event_indices2[mask]
    max_pairs_actual = len(event_indices1)

    # é‡‡æ ·ç²’å­
    indices1 = np.array([np.random.randint(0, len(event_list[i])) for i in event_indices1])
    indices2 = np.array([np.random.randint(0, len(event_list[j])) for j in event_indices2])

    batch_size = 1000000
    with tqdm(total=max_pairs_actual, desc="Mixed pairs", unit='pairs',
              bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as pbar:

        for start in range(0, max_pairs_actual, batch_size):
            end = min(start + batch_size, max_pairs_actual)
            batch_size_actual = end - start

            eta1 = np.array([event_list[event_indices1[k]][indices1[k], 0] for k in range(start, end)])
            phi1 = np.array([event_list[event_indices1[k]][indices1[k], 1] for k in range(start, end)])
            eta2 = np.array([event_list[event_indices2[k]][indices2[k], 0] for k in range(start, end)])
            phi2 = np.array([event_list[event_indices2[k]][indices2[k], 1] for k in range(start, end)])

            delta_eta = eta1 - eta2
            delta_phi = phi1 - phi2
            delta_phi = (delta_phi + np.pi) % (2 * np.pi) - np.pi
            delta_phi = np.where(delta_phi < -np.pi/2, delta_phi + 2*np.pi, delta_phi)

            valid_mask = (
                (delta_eta >= eta_range[0]) & 
                (delta_eta <= eta_range[1]) & 
                (delta_phi >= phi_range[0]) & 
                (delta_phi <= phi_range[1])
            )

            if np.any(valid_mask):
                hist, _, _ = np.histogram2d(
                    delta_eta[valid_mask], delta_phi[valid_mask],
                    bins=[eta_bins, phi_bins], range=[eta_range, phi_range]
                )
                B_N += hist
                background_pairs += np.sum(valid_mask)

            pbar.update(batch_size_actual)
    
    # è®¡ç®—åæ ‡è½´
    eta_centers = np.linspace(eta_range[0], eta_range[1], eta_bins, endpoint=False) + \
                  (eta_range[1] - eta_range[0]) / (2 * eta_bins)
    phi_centers = np.linspace(phi_range[0], phi_range[1], phi_bins, endpoint=False) + \
                  (phi_range[1] - phi_range[0]) / (2 * phi_bins)
    
    print(f"ğŸ”¢ Signal pairs used: {signal_pairs:,}")
    print(f"ğŸ”¢ Background pairs used: {background_pairs:,}")
    
    # å½’ä¸€åŒ–å’Œè®¡ç®—å…³è”å‡½æ•°
    S_N_normalized = S_N.copy()
    B_N_normalized = B_N.copy()
    
    if signal_pairs > 0:
        S_N_normalized /= signal_pairs
    if background_pairs > 0:
        B_N_normalized /= background_pairs
    
    B_N_normalized[B_N_normalized == 0] = 1e-9  # é¿å…é™¤é›¶é”™è¯¯
    C = S_N_normalized / B_N_normalized
    
    # åº”ç”¨ZYAMæ–¹æ³•
    C = apply_zyam_method(C, eta_bins, phi_bins)
    
    # æ¸…ç†å†…å­˜
    del event_data
    gc.collect()
    
    return eta_centers, phi_centers, C, S_N, B_N

def plot_signal_histogram(eta_centers, phi_centers, S_N, pt_min, pt_max, eta_min, eta_max, output_path):
    """ç»˜åˆ¶ä¿¡å·ç›´æ–¹å›¾"""
    if eta_centers is None or phi_centers is None or S_N is None:
        print("âš ï¸ No signal data to plot")
        return
        
    ETA, PHI = np.meshgrid(eta_centers, phi_centers)
    Z = S_N.T
    
    # è£å‰ªæç«¯å€¼
    z_min, z_max = np.percentile(Z, [2, 98])
    Z_clipped = np.clip(Z, z_min, z_max)
    
    # åˆ›å»ºé«˜è´¨é‡çš„2Då›¾
    fig, ax = plt.subplots(figsize=(12, 10))
    
    # ä½¿ç”¨æ›´å¥½çš„é¢œè‰²æ˜ å°„å’Œç­‰é«˜çº¿
    im = ax.contourf(ETA, PHI, Z_clipped, levels=100, cmap='viridis', extend='both')
    
    # æ·»åŠ ç­‰é«˜çº¿
    contour = ax.contour(ETA, PHI, Z_clipped, levels=20, colors='white', alpha=0.3, linewidths=0.5)
    
    # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
    ax.set_xlabel(r'$\Delta\eta$', fontsize=16)
    ax.set_ylabel(r'$\Delta\phi$ (rad)', fontsize=16)
    title = f'Signal Distribution (Same Event Pairs)\n(pT âˆˆ [{pt_min}, {pt_max}] GeV, |Î·| âˆˆ [{eta_min}, {eta_max}])'
    ax.set_title(title, fontsize=18, pad=20)
    
    # è®¾ç½®åæ ‡è½´èŒƒå›´
    ax.set_ylim(3*np.pi/2, -np.pi/2)
    ax.set_xlim(-2.2, 2.2)
    
    # æ·»åŠ ç½‘æ ¼
    ax.grid(True, linestyle='--', alpha=0.3)
    
    # æ·»åŠ é¢œè‰²æ¡
    cbar = plt.colorbar(im, ax=ax, shrink=0.8, aspect=20)
    cbar.set_label('Signal Counts', fontsize=14)
    
    # ä¼˜åŒ–å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜é«˜è´¨é‡å›¾ç‰‡
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"ğŸ–¼ï¸ Signal histogram saved to: {output_path}")

def plot_background_histogram(eta_centers, phi_centers, B_N, pt_min, pt_max, eta_min, eta_max, output_path):
    """ç»˜åˆ¶èƒŒæ™¯ç›´æ–¹å›¾"""
    if eta_centers is None or phi_centers is None or B_N is None:
        print("âš ï¸ No background data to plot")
        return
        
    ETA, PHI = np.meshgrid(eta_centers, phi_centers)
    Z = B_N.T
    
    # è£å‰ªæç«¯å€¼
    z_min, z_max = np.percentile(Z, [2, 98])
    Z_clipped = np.clip(Z, z_min, z_max)
    
    # åˆ›å»ºé«˜è´¨é‡çš„2Då›¾
    fig, ax = plt.subplots(figsize=(12, 10))
    
    # ä½¿ç”¨æ›´å¥½çš„é¢œè‰²æ˜ å°„å’Œç­‰é«˜çº¿
    im = ax.contourf(ETA, PHI, Z_clipped, levels=100, cmap='plasma', extend='both')
    
    # æ·»åŠ ç­‰é«˜çº¿
    contour = ax.contour(ETA, PHI, Z_clipped, levels=20, colors='white', alpha=0.3, linewidths=0.5)
    
    # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
    ax.set_xlabel(r'$\Delta\eta$', fontsize=16)
    ax.set_ylabel(r'$\Delta\phi$ (rad)', fontsize=16)
    title = f'Background Distribution (Mixed Event Pairs)\n(pT âˆˆ [{pt_min}, {pt_max}] GeV, |Î·| âˆˆ [{eta_min}, {eta_max}])'
    ax.set_title(title, fontsize=18, pad=20)
    
    # è®¾ç½®åæ ‡è½´èŒƒå›´
    ax.set_ylim(3*np.pi/2, -np.pi/2)
    ax.set_xlim(-2.2, 2.2)
    
    # æ·»åŠ ç½‘æ ¼
    ax.grid(True, linestyle='--', alpha=0.3)
    
    # æ·»åŠ é¢œè‰²æ¡
    cbar = plt.colorbar(im, ax=ax, shrink=0.8, aspect=20)
    cbar.set_label('Background Counts', fontsize=14)
    
    # ä¼˜åŒ–å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜é«˜è´¨é‡å›¾ç‰‡
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"ğŸ–¼ï¸ Background histogram saved to: {output_path}")

def plot_3d_correlation_enhanced(eta_centers, phi_centers, C, pt_min, pt_max, eta_min, eta_max, output_path):
    """ç»˜åˆ¶å¢å¼ºçš„3Då…³è”å›¾"""
    if eta_centers is None or phi_centers is None or C is None:
        print("âš ï¸ No data to plot")
        return
        
    ETA, PHI = np.meshgrid(eta_centers, phi_centers)
    Z = C.T
    
    # è£å‰ªæç«¯å€¼ - ä½¿ç”¨æ›´ä¿å®ˆçš„èŒƒå›´
    z_min, z_max = np.percentile(Z, [2, 98])
    Z_clipped = np.clip(Z, z_min, z_max)
    
    # åˆ›å»ºé«˜è´¨é‡çš„3Då›¾
    fig = plt.figure(figsize=(16, 12))
    ax = fig.add_subplot(111, projection='3d')
    
    # ä½¿ç”¨æ›´å¥½çš„é¢œè‰²æ˜ å°„
    surf = ax.plot_surface(
        ETA, PHI, Z_clipped,
        cmap='viridis', alpha=0.9, edgecolor='none', 
        rstride=1, cstride=1, antialiased=True,
        linewidth=0.1
    )
    
    # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
    ax.set_xlabel(r'$\Delta\eta$', fontsize=16, labelpad=20)
    ax.set_ylabel(r'$\Delta\phi$ (rad)', fontsize=16, labelpad=20)
    ax.set_zlabel(r'$C(\Delta\eta, \Delta\phi)$', fontsize=16, labelpad=20)
    
    title = f'Particle Angular Correlation Function\n(pT âˆˆ [{pt_min}, {pt_max}] GeV, |Î·| âˆˆ [{eta_min}, {eta_max}])'
    ax.set_title(title, fontsize=18, pad=25)
    
    # è®¾ç½®åæ ‡è½´èŒƒå›´
    ax.set_ylim(3*np.pi/2, -np.pi/2)
    ax.set_xlim(-2.2, 2.2)
    
    # æ·»åŠ ç½‘æ ¼
    ax.grid(True, linestyle='--', alpha=0.3)
    
    # è®¾ç½®è§†è§’
    ax.view_init(elev=35, azim=45)
    
    # æ·»åŠ é¢œè‰²æ¡
    cbar = fig.colorbar(surf, ax=ax, shrink=0.8, aspect=20, pad=0.1)
    cbar.set_label(r'$C(\Delta\eta, \Delta\phi)$', fontsize=14)
    
    # ä¼˜åŒ–å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜é«˜è´¨é‡å›¾ç‰‡
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"ğŸ–¼ï¸ Enhanced 3D plot saved to: {output_path}")

def plot_2d_correlation_enhanced(eta_centers, phi_centers, C, pt_min, pt_max, eta_min, eta_max, output_path):
    """ç»˜åˆ¶å¢å¼ºçš„2Då…³è”å›¾"""
    if eta_centers is None or phi_centers is None or C is None:
        print("âš ï¸ No data to plot")
        return
        
    ETA, PHI = np.meshgrid(eta_centers, phi_centers)
    Z = C.T
    
    # è£å‰ªæç«¯å€¼
    z_min, z_max = np.percentile(Z, [2, 98])
    Z_clipped = np.clip(Z, z_min, z_max)
    
    # åˆ›å»ºé«˜è´¨é‡çš„2Då›¾
    fig, ax = plt.subplots(figsize=(12, 10))
    
    # ä½¿ç”¨æ›´å¥½çš„é¢œè‰²æ˜ å°„å’Œç­‰é«˜çº¿
    im = ax.contourf(ETA, PHI, Z_clipped, levels=100, cmap='viridis', extend='both')
    
    # æ·»åŠ ç­‰é«˜çº¿
    contour = ax.contour(ETA, PHI, Z_clipped, levels=20, colors='white', alpha=0.3, linewidths=0.5)
    
    # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
    ax.set_xlabel(r'$\Delta\eta$', fontsize=16)
    ax.set_ylabel(r'$\Delta\phi$ (rad)', fontsize=16)
    title = f'Particle Angular Correlation Function\n(pT âˆˆ [{pt_min}, {pt_max}] GeV, |Î·| âˆˆ [{eta_min}, {eta_max}])'
    ax.set_title(title, fontsize=18, pad=20)
    
    # è®¾ç½®åæ ‡è½´èŒƒå›´
    ax.set_ylim(3*np.pi/2, -np.pi/2)
    ax.set_xlim(-2.2, 2.2)
    
    # æ·»åŠ ç½‘æ ¼
    ax.grid(True, linestyle='--', alpha=0.3)
    
    # æ·»åŠ é¢œè‰²æ¡
    cbar = plt.colorbar(im, ax=ax, shrink=0.8, aspect=20)
    cbar.set_label(r'$C(\Delta\eta, \Delta\phi)$', fontsize=14)
    
    # ä¼˜åŒ–å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜é«˜è´¨é‡å›¾ç‰‡
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"ğŸ–¼ï¸ Enhanced 2D plot saved to: {output_path}")

def save_to_root_file(eta_centers, phi_centers, C, S_N, B_N, pt_min, pt_max, eta_min, eta_max, group_label, output_root):
    """å°†å…³è”å‡½æ•°æ•°æ®ä¿å­˜åˆ°ROOTæ–‡ä»¶"""
    if not ROOT_AVAILABLE:
        print("âš ï¸ ROOTåº“ä¸å¯ç”¨ï¼Œè·³è¿‡ROOTæ–‡ä»¶ç”Ÿæˆ")
        return
    
    if eta_centers is None or phi_centers is None or C is None:
        print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä»¥ä¿å­˜åˆ°ROOTæ–‡ä»¶")
        return
    
    try:
        print(f"ğŸ’¾ ä¿å­˜æ•°æ®åˆ°ROOTæ–‡ä»¶: {output_root}")
        
        # åˆ›å»ºROOTæ–‡ä»¶
        root_file = ROOT.TFile(output_root, "RECREATE")
        
        # åˆ›å»º2Dç›´æ–¹å›¾
        eta_bins = len(eta_centers)
        phi_bins = len(phi_centers)
        
        # ä¿¡å·åˆ†å¸ƒç›´æ–¹å›¾
        h_signal = ROOT.TH2D(
            f"h_signal_{group_label}", 
            f"Signal Distribution {group_label};#Delta#eta;#Delta#phi (rad);Counts",
            eta_bins, -2.2, 2.2,
            phi_bins, -np.pi/2, 3*np.pi/2
        )
        
        # èƒŒæ™¯åˆ†å¸ƒç›´æ–¹å›¾
        h_background = ROOT.TH2D(
            f"h_background_{group_label}", 
            f"Background Distribution {group_label};#Delta#eta;#Delta#phi (rad);Counts",
            eta_bins, -2.2, 2.2,
            phi_bins, -np.pi/2, 3*np.pi/2
        )
        
        # å…³è”å‡½æ•°ç›´æ–¹å›¾
        h_correlation = ROOT.TH2D(
            f"h_correlation_{group_label}", 
            f"Correlation Function {group_label};#Delta#eta;#Delta#phi (rad);C(#Delta#eta,#Delta#phi)",
            eta_bins, -2.2, 2.2,
            phi_bins, -np.pi/2, 3*np.pi/2
        )
        
        # å¡«å……ç›´æ–¹å›¾æ•°æ®
        for i in range(eta_bins):
            for j in range(phi_bins):
                h_signal.SetBinContent(i+1, j+1, S_N[i, j])
                h_background.SetBinContent(i+1, j+1, B_N[i, j])
                h_correlation.SetBinContent(i+1, j+1, C[i, j])
        
        # åˆ›å»º1DæŠ•å½±ç›´æ–¹å›¾
        h_eta_projection = ROOT.TH1D(
            f"h_eta_projection_{group_label}",
            f"#Delta#eta Projection {group_label};#Delta#eta;C(#Delta#eta)",
            eta_bins, -2.2, 2.2
        )
        
        h_phi_projection = ROOT.TH1D(
            f"h_phi_projection_{group_label}",
            f"#Delta#phi Projection {group_label};#Delta#phi (rad);C(#Delta#phi)",
            phi_bins, -np.pi/2, 3*np.pi/2
        )
        
        # è®¡ç®—æŠ•å½±ï¼ˆæ²¿phiå’Œetaæ–¹å‘ç§¯åˆ†ï¼‰
        for i in range(eta_bins):
            eta_sum = 0
            for j in range(phi_bins):
                eta_sum += C[i, j]
            h_eta_projection.SetBinContent(i+1, eta_sum / phi_bins)
        
        for j in range(phi_bins):
            phi_sum = 0
            for i in range(eta_bins):
                phi_sum += C[i, j]
            h_phi_projection.SetBinContent(j+1, phi_sum / eta_bins)
        
        # åˆ›å»ºTTreeå­˜å‚¨åŸå§‹æ•°æ®ç‚¹
        tree = ROOT.TTree(f"correlation_data_{group_label}", f"Correlation Data {group_label}")
        
        # å®šä¹‰åˆ†æ”¯å˜é‡
        delta_eta = np.array([0.0], dtype=np.float32)
        delta_phi = np.array([0.0], dtype=np.float32)
        correlation_value = np.array([0.0], dtype=np.float32)
        signal_count = np.array([0.0], dtype=np.float32)
        background_count = np.array([0.0], dtype=np.float32)
        
        # åˆ›å»ºåˆ†æ”¯
        tree.Branch("delta_eta", delta_eta, "delta_eta/F")
        tree.Branch("delta_phi", delta_phi, "delta_phi/F")
        tree.Branch("correlation_value", correlation_value, "correlation_value/F")
        tree.Branch("signal_count", signal_count, "signal_count/F")
        tree.Branch("background_count", background_count, "background_count/F")
        
        # å¡«å……æ ‘
        for i in range(eta_bins):
            for j in range(phi_bins):
                delta_eta[0] = eta_centers[i]
                delta_phi[0] = phi_centers[j]
                correlation_value[0] = C[i, j]
                signal_count[0] = S_N[i, j]
                background_count[0] = B_N[i, j]
                tree.Fill()
        
        # åˆ›å»ºå…ƒæ•°æ®ç›´æ–¹å›¾
        h_metadata = ROOT.TH1D(f"h_metadata_{group_label}", "Analysis Parameters", 10, 0, 10)
        h_metadata.SetBinContent(1, pt_min)
        h_metadata.SetBinContent(2, pt_max)
        h_metadata.SetBinContent(3, eta_min)
        h_metadata.SetBinContent(4, eta_max)
        h_metadata.SetBinContent(5, eta_bins)
        h_metadata.SetBinContent(6, phi_bins)
        
        # è®¾ç½®å…ƒæ•°æ®æ ‡ç­¾
        h_metadata.GetXaxis().SetBinLabel(1, "pt_min")
        h_metadata.GetXaxis().SetBinLabel(2, "pt_max")
        h_metadata.GetXaxis().SetBinLabel(3, "eta_min")
        h_metadata.GetXaxis().SetBinLabel(4, "eta_max")
        h_metadata.GetXaxis().SetBinLabel(5, "eta_bins")
        h_metadata.GetXaxis().SetBinLabel(6, "phi_bins")
        
        # å†™å…¥æ–‡ä»¶
        h_signal.Write()
        h_background.Write()
        h_correlation.Write()
        h_eta_projection.Write()
        h_phi_projection.Write()
        h_metadata.Write()
        tree.Write()
        
        # å…³é—­æ–‡ä»¶
        root_file.Close()
        
        print(f"âœ… ROOTæ–‡ä»¶ä¿å­˜æˆåŠŸ: {output_root}")
        print(f"ğŸ“Š åŒ…å«å†…å®¹:")
        print(f"   - ä¿¡å·åˆ†å¸ƒç›´æ–¹å›¾: h_signal_{group_label}")
        print(f"   - èƒŒæ™¯åˆ†å¸ƒç›´æ–¹å›¾: h_background_{group_label}")
        print(f"   - å…³è”å‡½æ•°ç›´æ–¹å›¾: h_correlation_{group_label}")
        print(f"   - Î”Î·æŠ•å½±: h_eta_projection_{group_label}")
        print(f"   - Î”Ï†æŠ•å½±: h_phi_projection_{group_label}")
        print(f"   - å…ƒæ•°æ®: h_metadata_{group_label}")
        print(f"   - åŸå§‹æ•°æ®æ ‘: correlation_data_{group_label}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜ROOTæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

def process_all_multiplicity_groups():
    """å¤„ç†æ‰€æœ‰5ä¸ªå¤šé‡åº¦åŒºé—´çš„æ–‡ä»¶"""
    print("="*80)
    print("ğŸš€ æ‰¹é‡å¤„ç†æ‰€æœ‰å¤šé‡åº¦åŒºé—´æ–‡ä»¶")
    print("="*80)
    
    # è·å–ç”¨æˆ·è¾“å…¥çš„pTèŒƒå›´
    print("ğŸ“ è¯·è¾“å…¥pTèŒƒå›´...")
    while True:
        try:
            pt_min = float(input("pT_min (GeV): "))
            pt_max = float(input("pT_max (GeV): "))
            if pt_min >= pt_max:
                print("âŒ pTæœ€å°å€¼å¿…é¡»å°äºpTæœ€å¤§å€¼ï¼è¯·é‡æ–°è¾“å…¥")
                continue
            if pt_min < 0 or pt_max < 0:
                print("âŒ pTå€¼å¿…é¡»ä¸ºæ­£æ•°ï¼è¯·é‡æ–°è¾“å…¥")
                continue
            break
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—ï¼è¯·é‡æ–°è¾“å…¥")
    
    print(f"âœ… ç”¨æˆ·è®¾ç½®çš„pTèŒƒå›´: [{pt_min}, {pt_max}] GeV")
    
    # åªå¤„ç†å¤šé‡åº¦80%-100%çš„æ–‡ä»¶
    multiplicity_files = [
        "DDDAA/multiplicity_group_80-100_percent.txt"
    ]
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    existing_files = []
    for file_path in multiplicity_files:
        if os.path.exists(file_path):
            existing_files.append(file_path)
            print(f"âœ… æ‰¾åˆ°æ–‡ä»¶: {file_path}")
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    if not existing_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¤šé‡åº¦åˆ†ç»„æ–‡ä»¶ï¼")
        print("è¯·å…ˆè¿è¡Œ extract_high_multiplicity.py ç”Ÿæˆå¤šé‡åº¦åˆ†ç»„æ–‡ä»¶")
        return
    
    print(f"\nğŸ“Š å°†å¤„ç† {len(existing_files)} ä¸ªå¤šé‡åº¦åŒºé—´æ–‡ä»¶")
    
    # ä½¿ç”¨ç”¨æˆ·è¾“å…¥çš„pTèŒƒå›´
    eta_min = -1.1
    eta_max = 1.1
    max_pairs = 10000000
    
    total_start_time = time.time()
    
    for i, data_file in enumerate(existing_files):
        print(f"\n{'='*60}")
        print(f"ğŸ“ å¤„ç†æ–‡ä»¶ {i+1}/{len(existing_files)}: {os.path.basename(data_file)}")
        print(f"{'='*60}")
        
        try:
            # åŠ è½½æ•°æ®
            event_data = load_data_from_txt(data_file, pt_min, pt_max, eta_min, eta_max)
            if not event_data:
                print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆäº‹ä»¶ï¼Œè·³è¿‡æ­¤æ–‡ä»¶")
                continue
            
            # åˆ†æå¤šé‡åº¦åˆ†å¸ƒ
            analyze_multiplicity_distribution(event_data)
            
            # è®¡ç®—å…³è”å‡½æ•°å’Œç›´æ–¹å›¾
            eta, phi, C, S_N, B_N = calculate_correlation_with_histograms(
                event_data, eta_bins=22, phi_bins=22, max_pairs=max_pairs
            )
            
            if eta is not None:
                # ä»æ–‡ä»¶åæå–å¤šé‡åº¦åŒºé—´æ ‡ç­¾
                base = os.path.basename(data_file)
                group_label = base.split("multiplicity_group_")[1].split("_percent")[0]
                
                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å - ä½¿ç”¨å˜é‡PtèŒƒå›´
                prefix = f"multiplicity_{group_label}_pt{pt_min}-{pt_max}_eta-{eta_min}-{eta_max}_deltaEta2.2_bins22"
                output_3d = f"{prefix}_3D.png"
                output_2d = f"{prefix}_2D.png"
                output_signal = f"signal_{prefix}.png"
                output_background = f"background_{prefix}.png"
                output_root = f"{prefix}.root"
                
                # ç»˜åˆ¶ä¿¡å·å’ŒèƒŒæ™¯ç›´æ–¹å›¾
                plot_signal_histogram(eta, phi, S_N, pt_min, pt_max, eta_min, eta_max, output_signal)
                plot_background_histogram(eta, phi, B_N, pt_min, pt_max, eta_min, eta_max, output_background)
                
                # ç»˜åˆ¶å…³è”å‡½æ•°
                plot_3d_correlation_enhanced(eta, phi, C, pt_min, pt_max, eta_min, eta_max, output_3d)
                plot_2d_correlation_enhanced(eta, phi, C, pt_min, pt_max, eta_min, eta_max, output_2d)
                
                # ä¿å­˜ROOTæ–‡ä»¶
                save_to_root_file(eta, phi, C, S_N, B_N, pt_min, pt_max, eta_min, eta_max, group_label, output_root)
                
                print(f"âœ… æ–‡ä»¶ {group_label} å¤„ç†å®Œæˆ")
                print(f"ğŸ“Š ç”Ÿæˆå›¾ç‰‡: {output_3d}, {output_2d}, {output_signal}, {output_background}")
                print(f"ğŸ’¾ ç”ŸæˆROOTæ–‡ä»¶: {output_root}")
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶ {data_file} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    total_elapsed = time.time() - total_start_time
    print(f"\n{'='*60}")
    print("âœ… æ‰€æœ‰å¤šé‡åº¦åŒºé—´æ–‡ä»¶å¤„ç†å®Œæˆï¼")
    print(f"{'='*60}")
    print(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {total_elapsed/60:.1f} åˆ†é’Ÿ")
    print(f"ğŸ“Š å¤„ç†äº† {len(existing_files)} ä¸ªæ–‡ä»¶")
    if ROOT_AVAILABLE:
        print(f"ğŸ’¾ ç”Ÿæˆäº†ROOTæ–‡ä»¶ï¼Œå¯åœ¨ROOTä¸­è¿›ä¸€æ­¥åˆ†æ")

def main():
    # å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="Enhanced particle correlation analysis")
    parser.add_argument(
        "--data",
        nargs="+",
        default=["DDDAA/multiplicity_group_0-20_percent.txt"],
        help="One or more data files to process"
    )
    parser.add_argument(
        "--format",
        choices=["auto", "txt", "csv"],
        default="auto",
        help="Input file format: auto-detect, txt (space-separated), or csv (comma-separated)"
    )
    parser.add_argument(
        "--all-multiplicity",
        action="store_true",
        help="Process all multiplicity group files automatically"
    )
    args = parser.parse_args()

    # å¦‚æœæŒ‡å®šäº†--all-multiplicityï¼Œåˆ™æ‰¹é‡å¤„ç†æ‰€æœ‰å¤šé‡åº¦æ–‡ä»¶
    if args.all_multiplicity:
        process_all_multiplicity_groups()
        return

    # è·å–ç”¨æˆ·è¾“å…¥çš„pTèŒƒå›´
    print("ğŸ“ è¯·è¾“å…¥pTèŒƒå›´...")
    while True:
        try:
            pt_min = float(input("pT_min (GeV): "))
            pt_max = float(input("pT_max (GeV): "))
            if pt_min >= pt_max:
                print("âŒ pTæœ€å°å€¼å¿…é¡»å°äºpTæœ€å¤§å€¼ï¼è¯·é‡æ–°è¾“å…¥")
                continue
            if pt_min < 0 or pt_max < 0:
                print("âŒ pTå€¼å¿…é¡»ä¸ºæ­£æ•°ï¼è¯·é‡æ–°è¾“å…¥")
                continue
            break
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—ï¼è¯·é‡æ–°è¾“å…¥")
    
    print(f"âœ… ç”¨æˆ·è®¾ç½®çš„pTèŒƒå›´: [{pt_min}, {pt_max}] GeV")
    
    # ä½¿ç”¨ç”¨æˆ·è¾“å…¥çš„pTèŒƒå›´
    eta_min = -1.1
    eta_max = 1.1
    max_pairs = 10000000

    if MEMORY_MONITOR:
        print("ğŸ“Š Memory monitoring enabled")

    for data_file in args.data:
        if not os.path.exists(data_file):
            print(f"âŒ Error: File not found - {data_file}")
            continue

        print("="*80)
        print("ğŸš€ ENHANCED PARTICLE CORRELATION ANALYSIS WITH HISTOGRAMS")
        print("="*80)
        print(f"ğŸ“ File: {data_file}")
        print(f"ğŸ¯ pT range: [{pt_min}, {pt_max}] GeV")
        print(f"ğŸ¯ |Î·| range: [{eta_min}, {eta_max}]")
        print(f"ğŸ”¢ Max pairs: {max_pairs:,}")

        total_start_time = time.time()

        try:
            # ç¬¬ä¸€é˜¶æ®µï¼šåŠ è½½æ•°æ®ï¼ˆæ ¼å¼è‡ªé€‚åº”/æŒ‡å®šï¼‰
            input_format = args.format
            if input_format == "auto":
                # ç®€å•è‡ªåŠ¨åˆ¤æ–­ï¼šè‹¥æ–‡ä»¶åä»¥ .csv ç»“å°¾æˆ–æ–‡ä»¶å‰å‡ è¡ŒåŒ…å«é€—å·ï¼Œåˆ™è®¤ä¸º csv
                is_csv = data_file.lower().endswith('.csv')
                if not is_csv:
                    with open(data_file, 'r') as _f:
                        for _ in range(10):
                            l = _f.readline()
                            if not l:
                                break
                            if ',' in l:
                                is_csv = True
                                break
                input_format = 'csv' if is_csv else 'txt'

            if input_format == 'csv':
                event_data = load_data_from_pbpb_csv(data_file, pt_min, pt_max, eta_min, eta_max)
            else:
                event_data = load_data_from_txt(data_file, pt_min, pt_max, eta_min, eta_max)
            if not event_data:
                print("âš ï¸ No valid events found!")
                continue

            # åˆ†æå¤šé‡åº¦åˆ†å¸ƒ
            analyze_multiplicity_distribution(event_data)

            # ç¬¬äºŒé˜¶æ®µï¼šè®¡ç®—å…³è”å‡½æ•°å’Œç›´æ–¹å›¾
            eta, phi, C, S_N, B_N = calculate_correlation_with_histograms(
                event_data, eta_bins=22, phi_bins=22, max_pairs=max_pairs
            )

            if eta is not None:
                # æ¨æ–­è¾“å‡ºå‰ç¼€ä¸­çš„å¤šé‡åº¦åŒºé—´
                base = os.path.basename(data_file)
                group_label = ""
                if "multiplicity_group_" in base and "_percent" in base:
                    group_label = base.split("multiplicity_group_")[1].split("_percent")[0]
                else:
                    # é’ˆå¯¹ PbPb_20k.txt è‡ªå®šä¹‰å‰ç¼€
                    group_label = "custom"
                    if "pbpb" in base.lower():
                        group_label = "PbPb20k"

                # æ›´æ–°è¾“å‡ºæ–‡ä»¶åå‰ç¼€ - ä½¿ç”¨å˜é‡PtèŒƒå›´
                prefix = f"multiplicity_{group_label}_pt{pt_min}-{pt_max}_eta-{eta_min}-{eta_max}_deltaEta2.2_bins22"
                output_3d = f"{prefix}_3D.png"
                output_2d = f"{prefix}_2D.png"
                output_signal = f"signal_{prefix}.png"
                output_background = f"background_{prefix}.png"
                output_root = f"{prefix}.root"

                # ç»˜åˆ¶ä¿¡å·å’ŒèƒŒæ™¯ç›´æ–¹å›¾
                plot_signal_histogram(eta, phi, S_N, pt_min, pt_max, eta_min, eta_max, output_signal)
                plot_background_histogram(eta, phi, B_N, pt_min, pt_max, eta_min, eta_max, output_background)

                # ç»˜åˆ¶å…³è”å‡½æ•°
                plot_3d_correlation_enhanced(eta, phi, C, pt_min, pt_max, eta_min, eta_max, output_3d)
                plot_2d_correlation_enhanced(eta, phi, C, pt_min, pt_max, eta_min, eta_max, output_2d)
                
                # ä¿å­˜ROOTæ–‡ä»¶
                save_to_root_file(eta, phi, C, S_N, B_N, pt_min, pt_max, eta_min, eta_max, group_label, output_root)

            total_elapsed = time.time() - total_start_time
            print("\n" + "="*60)
            print("âœ… ENHANCED ANALYSIS COMPLETED SUCCESSFULLY")
            print("="*60)
            print(f"â±ï¸ Total processing time: {total_elapsed/60:.1f} minutes")
            print(f"ğŸ¯ Processed {len(event_data):,} events")
            print(f"ğŸ“Š Generated signal and background histograms")
            if ROOT_AVAILABLE:
                print(f"ğŸ’¾ Generated ROOT file for further analysis")

        except Exception as e:
            print(f"âŒ Error during processing: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    main()