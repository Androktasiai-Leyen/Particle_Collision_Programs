#!/usr/bin/env python3
"""
å¢å¼ºçš„ä¸¤ç²’å­å…³è”åˆ†æç¨‹åº - åŒ…å«ä¿¡å·å’ŒèƒŒæ™¯ç›´æ–¹å›¾ç»˜åˆ¶
é’ˆå¯¹ multiplicity_group_0-20_percent.txt æ•°æ®æ ¼å¼
pTèŒƒå›´ï¼š0.5-5.0 GeVï¼Œ|Î·| âˆˆ [-1.1, 1.1]
æ”¯æŒROOTæ–‡ä»¶è¾“å‡ºï¼Œä¾¿äºè¿›ä¸€æ­¥åˆ†æ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import cm
from tqdm.auto import tqdm
import os
import gc
import time
import argparse
import numba
from collections import defaultdict
import warnings
import pickle
import json
warnings.filterwarnings('ignore')

# å¯ç”¨å†…å­˜ç›‘æ§
try:
    import psutil
    MEMORY_MONITOR = True
except ImportError:
    MEMORY_MONITOR = False

# ROOTç›¸å…³å¯¼å…¥ - ä¿®å¤å¯¼å…¥é—®é¢˜
ROOT_AVAILABLE = False
try:
    import ROOT
    ROOT_AVAILABLE = True
    print("âœ… ROOTåº“å¯ç”¨ï¼Œå°†ç”ŸæˆROOTæ–‡ä»¶")
except ImportError:
    print("âš ï¸ ROOTåº“ä¸å¯ç”¨ï¼Œå°†è·³è¿‡ROOTæ–‡ä»¶ç”Ÿæˆ")
    print("   è¯·å®‰è£…ROOT: https://root.cern/install/")
    print("   æˆ–è€…ä½¿ç”¨conda: conda install -c conda-forge root")
except Exception as e:
    print(f"âš ï¸ ROOTåº“å¯¼å…¥å‡ºé”™: {e}")
    print("   å°†è·³è¿‡ROOTæ–‡ä»¶ç”Ÿæˆ")

def save_results_to_file(results_data, output_file):
    """
    ä¿å­˜è®¡ç®—ç»“æœåˆ°æ–‡ä»¶ï¼Œæ”¯æŒpickleå’Œjsonæ ¼å¼
    
    å‚æ•°:
    results_data: åŒ…å«æ‰€æœ‰è®¡ç®—ç»“æœçš„å­—å…¸
    output_file: è¾“å‡ºæ–‡ä»¶å
    """
    try:
        if output_file.endswith('.pkl'):
            with open(output_file, 'wb') as f:
                pickle.dump(results_data, f)
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        elif output_file.endswith('.json'):
            # å°†numpyæ•°ç»„è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
            json_data = {}
            for key, value in results_data.items():
                if isinstance(value, np.ndarray):
                    json_data[key] = value.tolist()
                else:
                    json_data[key] = value
            
            with open(output_file, 'w') as f:
                json.dump(json_data, f, indent=2)
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        else:
            print(f"âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {output_file}")
    except Exception as e:
        print(f"âŒ ä¿å­˜ç»“æœæ—¶å‡ºé”™: {e}")

def load_results_from_file(input_file):
    """
    ä»æ–‡ä»¶åŠ è½½è®¡ç®—ç»“æœ
    
    å‚æ•°:
    input_file: è¾“å…¥æ–‡ä»¶å
    
    è¿”å›:
    results_data: åŒ…å«æ‰€æœ‰è®¡ç®—ç»“æœçš„å­—å…¸ï¼Œå¦‚æœåŠ è½½å¤±è´¥åˆ™è¿”å›None
    """
    try:
        if input_file.endswith('.pkl'):
            with open(input_file, 'rb') as f:
                results_data = pickle.load(f)
            print(f"ğŸ“‚ ç»“æœå·²ä»æ–‡ä»¶åŠ è½½: {input_file}")
            return results_data
        elif input_file.endswith('.json'):
            with open(input_file, 'r') as f:
                results_data = json.load(f)
            
            # å°†åˆ—è¡¨è½¬æ¢å›numpyæ•°ç»„
            for key, value in results_data.items():
                if isinstance(value, list) and key in ['eta', 'phi', 'C', 'S_N', 'B_N']:
                    results_data[key] = np.array(value)
            
            print(f"ğŸ“‚ ç»“æœå·²ä»æ–‡ä»¶åŠ è½½: {input_file}")
            return results_data
        else:
            print(f"âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {input_file}")
            return None
    except Exception as e:
        print(f"âŒ åŠ è½½ç»“æœæ—¶å‡ºé”™: {e}")
        return None

def check_existing_results(pt_min, pt_max, eta_min, eta_max):
    """
    æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è®¡ç®—ç»“æœæ–‡ä»¶
    
    å‚æ•°:
    pt_min, pt_max: pTèŒƒå›´
    eta_min, eta_max: etaèŒƒå›´
    
    è¿”å›:
    existing_files: å·²å­˜åœ¨çš„ç»“æœæ–‡ä»¶åˆ—è¡¨
    """
    base_pattern = f"results_pt{pt_min}-{pt_max}_eta{eta_min}-{eta_max}"
    existing_files = []
    
    # æ£€æŸ¥pickleæ–‡ä»¶
    pkl_file = f"{base_pattern}.pkl"
    if os.path.exists(pkl_file):
        existing_files.append(pkl_file)
    
    # æ£€æŸ¥jsonæ–‡ä»¶
    json_file = f"{base_pattern}.json"
    if os.path.exists(json_file):
        existing_files.append(json_file)
    
    return existing_files

def plot_only_mode():
    """
    åªç»˜å›¾æ¨¡å¼ï¼šä½¿ç”¨å·²å­˜åœ¨çš„ç»“æœæ–‡ä»¶è¿›è¡Œç»˜å›¾ï¼Œä¸é‡æ–°è®¡ç®—
    """
    print("="*80)
    print("ğŸ¨ åªç»˜å›¾æ¨¡å¼ï¼šä½¿ç”¨å·²å­˜åœ¨çš„ç»“æœæ–‡ä»¶")
    print("="*80)
    
    # è·å–ç”¨æˆ·è¾“å…¥çš„pTèŒƒå›´
    print("ğŸ“ è¯·è¾“å…¥pTèŒƒå›´...")
    while True:
        try:
            pt_min = float(input("pT_min (GeV): "))
            pt_max = float(input("pT_max (GeV): "))
            if pt_min >= pt_max:
                print("âŒ pTæœ€å°å€¼å¿…é¡»å°äºpTæœ€å¤§å€¼ï¼è¯·é‡æ–°è¾“å…¥")
                continue
            if pt_min < 0 or pt_max < 0:
                print("âŒ pTå€¼å¿…é¡»ä¸ºæ­£æ•°ï¼è¯·é‡æ–°è¾“å…¥")
                continue
            break
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—ï¼è¯·é‡æ–°è¾“å…¥")
    
    print(f"âœ… ç”¨æˆ·è®¾ç½®çš„pTèŒƒå›´: [{pt_min}, {pt_max}] GeV")
    
    # è®¾ç½®etaèŒƒå›´
    eta_min = -1.1
    eta_max = 1.1
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è®¡ç®—ç»“æœ
    existing_results = check_existing_results(pt_min, pt_max, eta_min, eta_max)
    if not existing_results:
        print(f"\nâŒ æ²¡æœ‰æ‰¾åˆ°pTèŒƒå›´[{pt_min}, {pt_max}] GeVçš„ç»“æœæ–‡ä»¶ï¼")
        print("è¯·å…ˆè¿è¡Œè®¡ç®—æ¨¡å¼ç”Ÿæˆç»“æœæ–‡ä»¶ï¼Œæˆ–æ£€æŸ¥pTèŒƒå›´æ˜¯å¦æ­£ç¡®")
        return
    
    print(f"\nğŸ“‚ å‘ç°ç»“æœæ–‡ä»¶:")
    for result_file in existing_results:
        print(f"   {result_file}")
    
    # é€‰æ‹©è¦ä½¿ç”¨çš„ç»“æœæ–‡ä»¶
    if len(existing_results) > 1:
        print(f"\nğŸ“‚ å¤šä¸ªç»“æœæ–‡ä»¶å¯ç”¨ï¼Œé€‰æ‹©è¦ä½¿ç”¨çš„æ–‡ä»¶:")
        for i, result_file in enumerate(existing_results):
            print(f"   {i+1}. {result_file}")
        
        while True:
            try:
                choice = int(input("è¯·è¾“å…¥é€‰æ‹© (1-{}): ".format(len(existing_results))))
                if 1 <= choice <= len(existing_results):
                    selected_file = existing_results[choice-1]
                    break
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
    else:
        selected_file = existing_results[0]
    
    print(f"âœ… é€‰æ‹©ä½¿ç”¨æ–‡ä»¶: {selected_file}")
    
    # åŠ è½½ç»“æœæ–‡ä»¶
    results_data = load_results_from_file(selected_file)
    if not results_data:
        print("âŒ åŠ è½½ç»“æœæ–‡ä»¶å¤±è´¥ï¼")
        return
    
    # éªŒè¯æ•°æ®å®Œæ•´æ€§
    if 'group_labels' not in results_data or 'group_data' not in results_data:
        print("âŒ ç»“æœæ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ï¼")
        return
    
    print(f"\nğŸ“Š å¼€å§‹ä¸º {len(results_data['group_labels'])} ä¸ªå¤šé‡åº¦ç»„ç»˜åˆ¶å›¾è¡¨...")
    
    total_start_time = time.time()
    
    # ä¸ºæ¯ä¸ªå¤šé‡åº¦ç»„ç»˜åˆ¶å›¾è¡¨
    for group_label in results_data['group_labels']:
        print(f"\nğŸ“Š ä¸ºå¤šé‡åº¦ç»„ {group_label} ç»˜åˆ¶å›¾è¡¨...")
        
        try:
            # è·å–è¯¥ç»„çš„æ•°æ®
            group_data = results_data['group_data'][group_label]
            eta = group_data['eta']
            phi = group_data['phi']
            C = group_data['C']
            S_N = group_data['S_N']
            B_N = group_data['B_N']
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            prefix = f"multiplicity_{group_label}_pt{pt_min}-{pt_max}_eta-{eta_min}-{eta_max}_deltaEta2.2_bins22"
            output_3d = f"{prefix}_3D.png"
            output_2d = f"{prefix}_2D.png"
            output_signal = f"signal_{prefix}.png"
            output_background = f"background_{prefix}.png"
            
            # ç»˜åˆ¶å›¾è¡¨
            plot_signal_histogram(eta, phi, S_N, pt_min, pt_max, eta_min, eta_max, output_signal)
            plot_background_histogram(eta, phi, B_N, pt_min, pt_max, eta_min, eta_max, output_background)
            plot_3d_correlation_enhanced(eta, phi, C, pt_min, pt_max, eta_min, eta_max, output_3d)
            plot_2d_correlation_enhanced(eta, phi, C, pt_min, pt_max, eta_min, eta_max, output_2d)
            
            print(f"âœ… {group_label} å›¾è¡¨ç»˜åˆ¶å®Œæˆ")
            print(f"ğŸ“Š ç”Ÿæˆå›¾ç‰‡: {output_3d}, {output_2d}, {output_signal}, {output_background}")
            
        except Exception as e:
            print(f"âŒ ä¸º {group_label} ç»˜åˆ¶å›¾è¡¨æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    total_elapsed = time.time() - total_start_time
    print(f"\n{'='*60}")
    print("âœ… åªç»˜å›¾æ¨¡å¼å®Œæˆï¼")
    print(f"{'='*60}")
    print(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {total_elapsed/60:.1f} åˆ†é’Ÿ")
    print(f"ğŸ“Š å¤„ç†äº† {len(results_data['group_labels'])} ä¸ªå¤šé‡åº¦ç»„")
    print("ğŸ’¡ æç¤ºï¼šä¸‹æ¬¡è¿è¡Œæ—¶å¯ä½¿ç”¨ --plot-only å‚æ•°ç›´æ¥ç»˜å›¾ï¼Œæ— éœ€é‡æ–°è®¡ç®—")

@numba.njit
def calculate_delta_phi(phi1, phi2):
    """å¿«é€Ÿè®¡ç®—è§’åº¦å·®å¹¶å½’ä¸€åŒ–åˆ°[-Ï€/2, 3Ï€/2]èŒƒå›´"""
    delta = phi1 - phi2
    delta = (delta + np.pi) % (2 * np.pi) - np.pi
    if delta < -np.pi/2:
        delta += 2 * np.pi
    return delta

@numba.njit(parallel=True)
def vectorized_delta_calculation(eta1, phi1, eta2, phi2):
    """å‘é‡åŒ–è®¡ç®—etaå’Œphiå·®å¼‚"""
    n = len(eta1)
    delta_eta = np.empty(n, dtype=np.float32)
    delta_phi = np.empty(n, dtype=np.float32)
    
    for i in numba.prange(n):
        delta_eta[i] = eta1[i] - eta2[i]
        delta_phi[i] = calculate_delta_phi(phi1[i], phi2[i])
    
    return delta_eta, delta_phi

def load_data_from_txt(data_file, pt_min=1.0, pt_max=3.0, eta_min=-1.1, eta_max=1.1):
    """
    ä»txtæ–‡ä»¶åŠ è½½æ•°æ®ï¼Œæ ¼å¼ï¼ševent_id particle_id particle_type pt phi eta
    """
    print(f"ğŸ“‚ Loading data from: {os.path.basename(data_file)}")
    print(f"ğŸ¯ pT range: [{pt_min}, {pt_max}] GeV")
    print(f"ğŸ¯ |Î·| range: [{eta_min}, {eta_max}]")
    
    # å­˜å‚¨äº‹ä»¶æ•°æ®
    event_data = {}
    total_particles = 0
    filtered_particles = 0
    
    # è·å–æ–‡ä»¶å¤§å°
    file_size = os.path.getsize(data_file)
    file_size_mb = file_size / (1024**2)
    print(f"ğŸ“ File size: {file_size_mb:.2f} MB")
    
    # è¯»å–æ•°æ®
    with open(data_file, 'r') as f:
        for line_num, line in enumerate(f):
            if not line.strip():
                continue
                
            parts = line.strip().split()
            if len(parts) != 6:
                continue
            
            try:
                event_id = parts[0]  # ä¿æŒä¸ºå­—ç¬¦ä¸²
                particle_id = int(parts[1])
                particle_type = int(parts[2])
                pt = float(parts[3])
                phi = float(parts[4])
                eta = float(parts[5])
                
                total_particles += 1
                
                # åº”ç”¨ç­›é€‰æ¡ä»¶
                if (pt_min <= pt <= pt_max and eta_min <= abs(eta) <= eta_max):
                    filtered_particles += 1
                    
                    if event_id not in event_data:
                        event_data[event_id] = []
                    
                    event_data[event_id].append([eta, phi, pt, particle_type])
                
            except (ValueError, IndexError):
                continue
            
            # æ˜¾ç¤ºè¿›åº¦
            if (line_num + 1) % 100000 == 0:
                print(f"ğŸ“Š Processed {line_num + 1:,} lines, found {len(event_data):,} events")
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    for event_id in event_data:
        event_data[event_id] = np.array(event_data[event_id], dtype=np.float32)
    
    print(f"\nâœ… Data loading completed:")
    print(f"ğŸ“ˆ Total particles: {total_particles:,}")
    print(f"ğŸ“Š Filtered particles: {filtered_particles:,}")
    print(f"ğŸ“Š Events with valid particles: {len(event_data):,}")
    
    return event_data



def analyze_multiplicity_distribution(event_data):
    """
    åˆ†æå¤šé‡åº¦åˆ†å¸ƒï¼Œç”¨äºéªŒè¯äº‹ä»¶æ··åˆ
    """
    print(f"\nğŸ“Š Analyzing multiplicity distribution...")
    
    multiplicities = [len(particles) for particles in event_data.values()]
    multiplicities = np.array(multiplicities)
    
    print(f"ğŸ“ˆ Multiplicity statistics:")
    print(f"   Mean: {np.mean(multiplicities):.2f}")
    print(f"   Std: {np.std(multiplicities):.2f}")
    print(f"   Min: {np.min(multiplicities)}")
    print(f"   Max: {np.max(multiplicities)}")
    print(f"   Events with mult >= 3: {np.sum(multiplicities >= 3):,}")
    print(f"   Events with mult >= 5: {np.sum(multiplicities >= 5):,}")
    
    # ç»˜åˆ¶å¤šé‡åº¦åˆ†å¸ƒ
    plt.figure(figsize=(10, 6))
    plt.hist(multiplicities, bins=50, alpha=0.7, edgecolor='black')
    plt.xlabel('Event Multiplicity')
    plt.ylabel('Number of Events')
    plt.title('Event Multiplicity Distribution')
    plt.yscale('log')
    plt.grid(True, alpha=0.3)
    plt.savefig('multiplicity_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ–¼ï¸ Multiplicity distribution saved to: multiplicity_distribution.png")
    
    return multiplicities



def calculate_correlation_with_histograms(event_data, eta_bins=22, phi_bins=22, max_pairs=50000000):
    """
    è®¡ç®—å…³è”å‡½æ•°å¹¶è¿”å›ä¿¡å·å’ŒèƒŒæ™¯ç›´æ–¹å›¾
    ä¸¥æ ¼æŒ‰ç…§å›¾ç‰‡ä¸­çš„å…¬å¼ï¼š
    S(Î”Î·, Î”Ï†) = (1/N_trig) Ã— dÂ²N_same/(dÎ”Î· dÎ”Ï†)
    B(Î”Î·, Î”Ï†) = Î± Ã— dÂ²N_mixed/(dÎ”Î· dÎ”Ï†) å…¶ä¸­ B(0,0) = 1
    C(Î”Î·, Î”Ï†) = S(Î”Î·, Î”Ï†) / B(Î”Î·, Î”Ï†)
    """
    print(f"\nğŸ“Š Calculating correlation function...")
    print(f"ğŸ”¢ Grid: {eta_bins} Ã— {phi_bins}")
    print(f"ğŸ”¢ Max pairs: {max_pairs:,}")
    
    # å®šä¹‰èŒƒå›´
    eta_range = (-2.2, 2.2)  # ä¿®æ”¹ï¼šÎ”Î· èŒƒå›´è®¾ç½®ä¸º[-2.2,2.2]
    phi_range = (-np.pi/2, 3*np.pi/2)
    
    # åˆå§‹åŒ–ç›´æ–¹å›¾
    S_N = np.zeros((eta_bins, phi_bins), dtype=np.float64)
    B_N = np.zeros((eta_bins, phi_bins), dtype=np.float64)
    
    if not event_data:
        print("âš ï¸ No event data found!")
        return None, None, None, None, None
    
    n_events = len(event_data)
    event_list = list(event_data.values())
    
    print(f"ğŸ¯ Total events: {n_events:,}")
    
    # è®¡ç®—æ€»è§¦å‘ç²’å­æ•° N_trig
    N_trig = sum(len(particles) for particles in event_list)
    print(f"ğŸ¯ Total trigger particles: {N_trig:,}")
    
    # ä¿¡å·åˆ†å¸ƒ - å‘é‡åŒ–è®¡ç®—
    print("ğŸ” Computing signal distribution...")
    signal_pairs = 0
    np.random.seed(42)
    
    # é¢„è®¡ç®—äº‹ä»¶æƒé‡
    event_weights = np.array([len(particles) for particles in event_list], dtype=np.float64)
    event_probs = event_weights / event_weights.sum()
    
    # é‡‡æ ·äº‹ä»¶
    sampled_events = np.random.choice(n_events, size=max_pairs, p=event_probs)
    event_counts = np.bincount(sampled_events, minlength=n_events)
    
    with tqdm(total=len(event_counts), desc="Signal pairs", unit='events',
              bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as pbar:
        
        for i, count in enumerate(event_counts):
            if count == 0:
                pbar.update(1)
                continue
                
            particles = event_list[i]
            n_particles = len(particles)
            
            if n_particles < 2:
                pbar.update(1)
                continue
                
            # è®¡ç®—è¯¥äº‹ä»¶ä¸­éœ€è¦é‡‡æ ·çš„å¯¹æ•°
            n_pairs = min(count, n_particles * (n_particles - 1) // 2)
            if n_pairs == 0:
                pbar.update(1)
                continue
                
            # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„ç²’å­å¯¹ç´¢å¼•
            indices = np.triu_indices(n_particles, k=1)
            if n_pairs < len(indices[0]):
                selected = np.random.choice(len(indices[0]), size=n_pairs, replace=False)
                i_indices = indices[0][selected]
                j_indices = indices[1][selected]
            else:
                i_indices = indices[0]
                j_indices = indices[1]
            
            # æå–ç²’å­åæ ‡
            particles_arr = particles
            eta_i = particles_arr[i_indices, 0]
            phi_i = particles_arr[i_indices, 1]
            eta_j = particles_arr[j_indices, 0]
            phi_j = particles_arr[j_indices, 1]
            
            # è®¡ç®—å·®å¼‚
            delta_eta = eta_i - eta_j
            delta_phi = phi_i - phi_j
            
            # å‘é‡åŒ–phiå½’ä¸€åŒ–
            delta_phi = (delta_phi + np.pi) % (2 * np.pi) - np.pi
            delta_phi = np.where(delta_phi < -np.pi/2, delta_phi + 2*np.pi, delta_phi)
            
            # å‘é‡åŒ–ç›´æ–¹å›¾
            valid_mask = (
                (delta_eta >= eta_range[0]) & 
                (delta_eta <= eta_range[1]) & 
                (delta_phi >= phi_range[0]) & 
                (delta_phi <= phi_range[1])
            )
            
            if np.any(valid_mask):
                hist, _, _ = np.histogram2d(
                    delta_eta[valid_mask], delta_phi[valid_mask],
                    bins=[eta_bins, phi_bins], range=[eta_range, phi_range]
                )
                S_N += hist
                signal_pairs += np.sum(valid_mask)
            
            pbar.update(1)
    
    # èƒŒæ™¯åˆ†å¸ƒ - å‘é‡åŒ–æ··åˆäº‹ä»¶
    print("ğŸ” Computing background distribution...")
    background_pairs = 0

    # é‡‡æ ·äº‹ä»¶å¯¹ï¼Œä¿è¯ä¸åŒäº‹ä»¶
    np.random.seed(42)
    event_indices1 = np.random.randint(0, n_events, size=max_pairs)
    event_indices2 = np.random.randint(0, n_events, size=max_pairs)
    # ä¿è¯ event_indices1 != event_indices2
    mask = event_indices1 != event_indices2
    event_indices1 = event_indices1[mask]
    event_indices2 = event_indices2[mask]
    max_pairs_actual = len(event_indices1)

    # é‡‡æ ·ç²’å­
    indices1 = np.array([np.random.randint(0, len(event_list[i])) for i in event_indices1])
    indices2 = np.array([np.random.randint(0, len(event_list[j])) for j in event_indices2])

    batch_size = 1000000
    with tqdm(total=max_pairs_actual, desc="Mixed pairs", unit='pairs',
              bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as pbar:

        for start in range(0, max_pairs_actual, batch_size):
            end = min(start + batch_size, max_pairs_actual)
            batch_size_actual = end - start

            eta1 = np.array([event_list[event_indices1[k]][indices1[k], 0] for k in range(start, end)])
            phi1 = np.array([event_list[event_indices1[k]][indices1[k], 1] for k in range(start, end)])
            eta2 = np.array([event_list[event_indices2[k]][indices2[k], 0] for k in range(start, end)])
            phi2 = np.array([event_list[event_indices2[k]][indices2[k], 1] for k in range(start, end)])

            delta_eta = eta1 - eta2
            delta_phi = phi1 - phi2
            delta_phi = (delta_phi + np.pi) % (2 * np.pi) - np.pi
            delta_phi = np.where(delta_phi < -np.pi/2, delta_phi + 2*np.pi, delta_phi)

            valid_mask = (
                (delta_eta >= eta_range[0]) & 
                (delta_eta <= eta_range[1]) & 
                (delta_phi >= phi_range[0]) & 
                (delta_phi <= phi_range[1])
            )

            if np.any(valid_mask):
                hist, _, _ = np.histogram2d(
                    delta_eta[valid_mask], delta_phi[valid_mask],
                    bins=[eta_bins, phi_bins], range=[eta_range, phi_range]
                )
                B_N += hist
                background_pairs += np.sum(valid_mask)

            pbar.update(batch_size_actual)
    
    # è®¡ç®—åæ ‡è½´
    eta_centers = np.linspace(eta_range[0], eta_range[1], eta_bins, endpoint=False) + \
                  (eta_range[1] - eta_range[0]) / (2 * eta_bins)
    phi_centers = np.linspace(phi_range[0], phi_range[1], phi_bins, endpoint=False) + \
                  (phi_range[1] - phi_range[0]) / (2 * phi_bins)
    
    print(f"ğŸ”¢ Signal pairs used: {signal_pairs:,}")
    print(f"ğŸ”¢ Background pairs used: {background_pairs:,}")
    
    # æŒ‰ç…§å›¾ç‰‡ä¸­çš„å…¬å¼è¿›è¡Œå½’ä¸€åŒ–
    print("ğŸ” Applying normalization according to the formulas in the image...")
    
    # 1. ä¿¡å·åˆ†å¸ƒå½’ä¸€åŒ–: S(Î”Î·, Î”Ï†) = (1/N_trig) Ã— dÂ²N_same/(dÎ”Î· dÎ”Ï†)
    S_normalized = S_N / N_trig
    
    # 2. èƒŒæ™¯åˆ†å¸ƒå½’ä¸€åŒ–: B(Î”Î·, Î”Ï†) = Î± Ã— dÂ²N_mixed/(dÎ”Î· dÎ”Ï†) å…¶ä¸­ B(0,0) = 1
    # é¦–å…ˆæ‰¾åˆ° Î”Î·=0, Î”Ï†=0 å¯¹åº”çš„binç´¢å¼•
    eta_zero_idx = np.argmin(np.abs(eta_centers))
    phi_zero_idx = np.argmin(np.abs(phi_centers))
    
    # è®¡ç®— Î± å› å­ï¼Œä½¿å¾— B(0,0) = 1
    if B_N[eta_zero_idx, phi_zero_idx] > 0:
        alpha = 1.0 / B_N[eta_zero_idx, phi_zero_idx]
        print(f"ğŸ” Background normalization factor Î± = {alpha:.6f}")
        print(f"ğŸ” B(0,0) before normalization: {B_N[eta_zero_idx, phi_zero_idx]:.6f}")
        print(f"ğŸ” B(0,0) after normalization: {alpha * B_N[eta_zero_idx, phi_zero_idx]:.6f}")
    else:
        # å¦‚æœ B(0,0) = 0ï¼Œä½¿ç”¨æ•´ä½“å½’ä¸€åŒ–
        alpha = 1.0 / np.max(B_N) if np.max(B_N) > 0 else 1.0
        print(f"âš ï¸ B(0,0) = 0, using alternative normalization Î± = {alpha:.6f}")
    
    B_normalized = alpha * B_N
    
    # 3. å…³è”å‡½æ•°: C(Î”Î·, Î”Ï†) = S(Î”Î·, Î”Ï†) / B(Î”Î·, Î”Ï†)
    # é¿å…é™¤é›¶é”™è¯¯
    B_normalized[B_normalized == 0] = 1e-9
    C = S_normalized / B_normalized
    

    
    # æ¸…ç†å†…å­˜
    del event_data
    gc.collect()
    
    return eta_centers, phi_centers, C, S_normalized, B_normalized

def plot_signal_histogram(eta_centers, phi_centers, S_N, pt_min, pt_max, eta_min, eta_max, output_path):
    """ç»˜åˆ¶ä¿¡å·ç›´æ–¹å›¾"""
    if eta_centers is None or phi_centers is None or S_N is None:
        print("âš ï¸ No signal data to plot")
        return
        
    ETA, PHI = np.meshgrid(eta_centers, phi_centers)
    Z = S_N.T
    
    # è£å‰ªæç«¯å€¼
    z_min, z_max = np.percentile(Z, [2, 98])
    Z_clipped = np.clip(Z, z_min, z_max)
    
    # åˆ›å»ºé«˜è´¨é‡çš„2Då›¾
    fig, ax = plt.subplots(figsize=(12, 10))
    
    # ä½¿ç”¨æ›´å¥½çš„é¢œè‰²æ˜ å°„å’Œç­‰é«˜çº¿
    im = ax.contourf(ETA, PHI, Z_clipped, levels=100, cmap='viridis', extend='both')
    
    # æ·»åŠ ç­‰é«˜çº¿
    contour = ax.contour(ETA, PHI, Z_clipped, levels=20, colors='white', alpha=0.3, linewidths=0.5)
    
    # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
    ax.set_xlabel(r'$\Delta\eta$', fontsize=16)
    ax.set_ylabel(r'$\Delta\phi$ (rad)', fontsize=16)
    title = f'Signal Distribution (Same Event Pairs)\n(pT âˆˆ [{pt_min}, {pt_max}] GeV, |Î·| âˆˆ [{eta_min}, {eta_max}])'
    ax.set_title(title, fontsize=18, pad=20)
    
    # è®¾ç½®åæ ‡è½´èŒƒå›´
    ax.set_ylim(3*np.pi/2, -np.pi/2)
    ax.set_xlim(-2.2, 2.2)
    
    # æ·»åŠ ç½‘æ ¼
    ax.grid(True, linestyle='--', alpha=0.3)
    
    # æ·»åŠ é¢œè‰²æ¡
    cbar = plt.colorbar(im, ax=ax, shrink=0.8, aspect=20)
    cbar.set_label('Signal Counts', fontsize=14)
    
    # ä¼˜åŒ–å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜é«˜è´¨é‡å›¾ç‰‡
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"ğŸ–¼ï¸ Signal histogram saved to: {output_path}")

def plot_background_histogram(eta_centers, phi_centers, B_N, pt_min, pt_max, eta_min, eta_max, output_path):
    """ç»˜åˆ¶èƒŒæ™¯ç›´æ–¹å›¾"""
    if eta_centers is None or phi_centers is None or B_N is None:
        print("âš ï¸ No background data to plot")
        return
        
    ETA, PHI = np.meshgrid(eta_centers, phi_centers)
    Z = B_N.T
    
    # è£å‰ªæç«¯å€¼
    z_min, z_max = np.percentile(Z, [2, 98])
    Z_clipped = np.clip(Z, z_min, z_max)
    
    # åˆ›å»ºé«˜è´¨é‡çš„2Då›¾
    fig, ax = plt.subplots(figsize=(12, 10))
    
    # ä½¿ç”¨æ›´å¥½çš„é¢œè‰²æ˜ å°„å’Œç­‰é«˜çº¿
    im = ax.contourf(ETA, PHI, Z_clipped, levels=100, cmap='plasma', extend='both')
    
    # æ·»åŠ ç­‰é«˜çº¿
    contour = ax.contour(ETA, PHI, Z_clipped, levels=20, colors='white', alpha=0.3, linewidths=0.5)
    
    # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
    ax.set_xlabel(r'$\Delta\eta$', fontsize=16)
    ax.set_ylabel(r'$\Delta\phi$ (rad)', fontsize=16)
    title = f'Background Distribution (Mixed Event Pairs)\n(pT âˆˆ [{pt_min}, {pt_max}] GeV, |Î·| âˆˆ [{eta_min}, {eta_max}])'
    ax.set_title(title, fontsize=18, pad=20)
    
    # è®¾ç½®åæ ‡è½´èŒƒå›´
    ax.set_ylim(3*np.pi/2, -np.pi/2)
    ax.set_xlim(-2.2, 2.2)
    
    # æ·»åŠ ç½‘æ ¼
    ax.grid(True, linestyle='--', alpha=0.3)
    
    # æ·»åŠ é¢œè‰²æ¡
    cbar = plt.colorbar(im, ax=ax, shrink=0.8, aspect=20)
    cbar.set_label('Background Counts', fontsize=14)
    
    # ä¼˜åŒ–å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜é«˜è´¨é‡å›¾ç‰‡
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"ğŸ–¼ï¸ Background histogram saved to: {output_path}")

def plot_3d_correlation_enhanced(eta_centers, phi_centers, C, pt_min, pt_max, eta_min, eta_max, output_path):
    """ç»˜åˆ¶å¢å¼ºçš„3Då…³è”å›¾"""
    if eta_centers is None or phi_centers is None or C is None:
        print("âš ï¸ No data to plot")
        return
        
    ETA, PHI = np.meshgrid(eta_centers, phi_centers)
    Z = C.T
    
    # è£å‰ªæç«¯å€¼ - ä½¿ç”¨æ›´ä¿å®ˆçš„èŒƒå›´
    z_min, z_max = np.percentile(Z, [2, 98])
    Z_clipped = np.clip(Z, z_min, z_max)
    
    # åˆ›å»ºé«˜è´¨é‡çš„3Då›¾
    fig = plt.figure(figsize=(16, 12))
    ax = fig.add_subplot(111, projection='3d')
    
    # ä½¿ç”¨æ›´å¥½çš„é¢œè‰²æ˜ å°„
    surf = ax.plot_surface(
        ETA, PHI, Z_clipped,
        cmap='viridis', alpha=0.9, edgecolor='none', 
        rstride=1, cstride=1, antialiased=True,
        linewidth=0.1
    )
    
    # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
    ax.set_xlabel(r'$\Delta\eta$', fontsize=16, labelpad=20)
    ax.set_ylabel(r'$\Delta\phi$ (rad)', fontsize=16, labelpad=20)
    ax.set_zlabel(r'$C(\Delta\eta, \Delta\phi)$', fontsize=16, labelpad=20)
    
    title = f'Particle Angular Correlation Function\n(pT âˆˆ [{pt_min}, {pt_max}] GeV, |Î·| âˆˆ [{eta_min}, {eta_max}])'
    ax.set_title(title, fontsize=18, pad=25)
    
    # è®¾ç½®åæ ‡è½´èŒƒå›´
    ax.set_ylim(3*np.pi/2, -np.pi/2)
    ax.set_xlim(-2.2, 2.2)
    
    # æ·»åŠ ç½‘æ ¼
    ax.grid(True, linestyle='--', alpha=0.3)
    
    # è®¾ç½®è§†è§’
    ax.view_init(elev=35, azim=45)
    
    # æ·»åŠ é¢œè‰²æ¡
    cbar = fig.colorbar(surf, ax=ax, shrink=0.8, aspect=20, pad=0.1)
    cbar.set_label(r'$C(\Delta\eta, \Delta\phi)$', fontsize=14)
    
    # ä¼˜åŒ–å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜é«˜è´¨é‡å›¾ç‰‡
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"ğŸ–¼ï¸ Enhanced 3D plot saved to: {output_path}")

def plot_2d_correlation_enhanced(eta_centers, phi_centers, C, pt_min, pt_max, eta_min, eta_max, output_path):
    """ç»˜åˆ¶å¢å¼ºçš„2Då…³è”å›¾"""
    if eta_centers is None or phi_centers is None or C is None:
        print("âš ï¸ No data to plot")
        return
        
    ETA, PHI = np.meshgrid(eta_centers, phi_centers)
    Z = C.T
    
    # è£å‰ªæç«¯å€¼
    z_min, z_max = np.percentile(Z, [2, 98])
    Z_clipped = np.clip(Z, z_min, z_max)
    
    # åˆ›å»ºé«˜è´¨é‡çš„2Då›¾
    fig, ax = plt.subplots(figsize=(12, 10))
    
    # ä½¿ç”¨æ›´å¥½çš„é¢œè‰²æ˜ å°„å’Œç­‰é«˜çº¿
    im = ax.contourf(ETA, PHI, Z_clipped, levels=100, cmap='viridis', extend='both')
    
    # æ·»åŠ ç­‰é«˜çº¿
    contour = ax.contour(ETA, PHI, Z_clipped, levels=20, colors='white', alpha=0.3, linewidths=0.5)
    
    # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
    ax.set_xlabel(r'$\Delta\eta$', fontsize=16)
    ax.set_ylabel(r'$\Delta\phi$ (rad)', fontsize=16)
    title = f'Particle Angular Correlation Function\n(pT âˆˆ [{pt_min}, {pt_max}] GeV, |Î·| âˆˆ [{eta_min}, {eta_max}])'
    ax.set_title(title, fontsize=18, pad=20)
    
    # è®¾ç½®åæ ‡è½´èŒƒå›´
    ax.set_ylim(3*np.pi/2, -np.pi/2)
    ax.set_xlim(-2.2, 2.2)
    
    # æ·»åŠ ç½‘æ ¼
    ax.grid(True, linestyle='--', alpha=0.3)
    
    # æ·»åŠ é¢œè‰²æ¡
    cbar = plt.colorbar(im, ax=ax, shrink=0.8, aspect=20)
    cbar.set_label(r'$C(\Delta\eta, \Delta\phi)$', fontsize=14)
    
    # ä¼˜åŒ–å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜é«˜è´¨é‡å›¾ç‰‡
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"ğŸ–¼ï¸ Enhanced 2D plot saved to: {output_path}")

def save_to_root_file(eta_centers, phi_centers, C, S_N, B_N, pt_min, pt_max, eta_min, eta_max, group_label, output_root):
    """å°†å…³è”å‡½æ•°æ•°æ®ä¿å­˜åˆ°ROOTæ–‡ä»¶"""
    if not ROOT_AVAILABLE:
        print("âš ï¸ ROOTåº“ä¸å¯ç”¨ï¼Œè·³è¿‡ROOTæ–‡ä»¶ç”Ÿæˆ")
        return
    
    if eta_centers is None or phi_centers is None or C is None:
        print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä»¥ä¿å­˜åˆ°ROOTæ–‡ä»¶")
        return
    
    try:
        print(f"ğŸ’¾ ä¿å­˜æ•°æ®åˆ°ROOTæ–‡ä»¶: {output_root}")
        
        # åˆ›å»ºROOTæ–‡ä»¶
        root_file = ROOT.TFile(output_root, "RECREATE")
        
        # åˆ›å»º2Dç›´æ–¹å›¾
        eta_bins = len(eta_centers)
        phi_bins = len(phi_centers)
        
        # ä¿¡å·åˆ†å¸ƒç›´æ–¹å›¾
        h_signal = ROOT.TH2D(
            f"h_signal_{group_label}", 
            f"Signal Distribution {group_label};#Delta#eta;#Delta#phi (rad);Counts",
            eta_bins, -2.2, 2.2,
            phi_bins, -np.pi/2, 3*np.pi/2
        )
        
        # èƒŒæ™¯åˆ†å¸ƒç›´æ–¹å›¾
        h_background = ROOT.TH2D(
            f"h_background_{group_label}", 
            f"Background Distribution {group_label};#Delta#eta;#Delta#phi (rad);Counts",
            eta_bins, -2.2, 2.2,
            phi_bins, -np.pi/2, 3*np.pi/2
        )
        
        # å…³è”å‡½æ•°ç›´æ–¹å›¾
        h_correlation = ROOT.TH2D(
            f"h_correlation_{group_label}", 
            f"Correlation Function {group_label};#Delta#eta;#Delta#phi (rad);C(#Delta#eta,#Delta#phi)",
            eta_bins, -2.2, 2.2,
            phi_bins, -np.pi/2, 3*np.pi/2
        )
        
        # å¡«å……ç›´æ–¹å›¾æ•°æ®
        for i in range(eta_bins):
            for j in range(phi_bins):
                h_signal.SetBinContent(i+1, j+1, S_N[i, j])
                h_background.SetBinContent(i+1, j+1, B_N[i, j])
                h_correlation.SetBinContent(i+1, j+1, C[i, j])
        
        # åˆ›å»º1DæŠ•å½±ç›´æ–¹å›¾
        h_eta_projection = ROOT.TH1D(
            f"h_eta_projection_{group_label}",
            f"#Delta#eta Projection {group_label};#Delta#eta;C(#Delta#eta)",
            eta_bins, -2.2, 2.2
        )
        
        h_phi_projection = ROOT.TH1D(
            f"h_phi_projection_{group_label}",
            f"#Delta#phi Projection {group_label};#Delta#phi (rad);C(#Delta#phi)",
            phi_bins, -np.pi/2, 3*np.pi/2
        )
        
        # è®¡ç®—æŠ•å½±ï¼ˆæ²¿phiå’Œetaæ–¹å‘ç§¯åˆ†ï¼‰
        for i in range(eta_bins):
            eta_sum = 0
            for j in range(phi_bins):
                eta_sum += C[i, j]
            h_eta_projection.SetBinContent(i+1, eta_sum / phi_bins)
        
        for j in range(phi_bins):
            phi_sum = 0
            for i in range(eta_bins):
                phi_sum += C[i, j]
            h_phi_projection.SetBinContent(j+1, phi_sum / eta_bins)
        
        # åˆ›å»ºTTreeå­˜å‚¨åŸå§‹æ•°æ®ç‚¹
        tree = ROOT.TTree(f"correlation_data_{group_label}", f"Correlation Data {group_label}")
        
        # å®šä¹‰åˆ†æ”¯å˜é‡
        delta_eta = np.array([0.0], dtype=np.float32)
        delta_phi = np.array([0.0], dtype=np.float32)
        correlation_value = np.array([0.0], dtype=np.float32)
        signal_count = np.array([0.0], dtype=np.float32)
        background_count = np.array([0.0], dtype=np.float32)
        
        # åˆ›å»ºåˆ†æ”¯
        tree.Branch("delta_eta", delta_eta, "delta_eta/F")
        tree.Branch("delta_phi", delta_phi, "delta_phi/F")
        tree.Branch("correlation_value", correlation_value, "correlation_value/F")
        tree.Branch("signal_count", signal_count, "signal_count/F")
        tree.Branch("background_count", background_count, "background_count/F")
        
        # å¡«å……æ ‘
        for i in range(eta_bins):
            for j in range(phi_bins):
                delta_eta[0] = eta_centers[i]
                delta_phi[0] = phi_centers[j]
                correlation_value[0] = C[i, j]
                signal_count[0] = S_N[i, j]
                background_count[0] = B_N[i, j]
                tree.Fill()
        
        # åˆ›å»ºå…ƒæ•°æ®ç›´æ–¹å›¾
        h_metadata = ROOT.TH1D(f"h_metadata_{group_label}", "Analysis Parameters", 10, 0, 10)
        h_metadata.SetBinContent(1, pt_min)
        h_metadata.SetBinContent(2, pt_max)
        h_metadata.SetBinContent(3, eta_min)
        h_metadata.SetBinContent(4, eta_max)
        h_metadata.SetBinContent(5, eta_bins)
        h_metadata.SetBinContent(6, phi_bins)
        
        # è®¾ç½®å…ƒæ•°æ®æ ‡ç­¾
        h_metadata.GetXaxis().SetBinLabel(1, "pt_min")
        h_metadata.GetXaxis().SetBinLabel(2, "pt_max")
        h_metadata.GetXaxis().SetBinLabel(3, "eta_min")
        h_metadata.GetXaxis().SetBinLabel(4, "eta_max")
        h_metadata.GetXaxis().SetBinLabel(5, "eta_bins")
        h_metadata.GetXaxis().SetBinLabel(6, "phi_bins")
        
        # å†™å…¥æ–‡ä»¶
        h_signal.Write()
        h_background.Write()
        h_correlation.Write()
        h_eta_projection.Write()
        h_phi_projection.Write()
        h_metadata.Write()
        tree.Write()
        
        # å…³é—­æ–‡ä»¶
        root_file.Close()
        
        print(f"âœ… ROOTæ–‡ä»¶ä¿å­˜æˆåŠŸ: {output_root}")
        print(f"ğŸ“Š åŒ…å«å†…å®¹:")
        print(f"   - ä¿¡å·åˆ†å¸ƒç›´æ–¹å›¾: h_signal_{group_label}")
        print(f"   - èƒŒæ™¯åˆ†å¸ƒç›´æ–¹å›¾: h_background_{group_label}")
        print(f"   - å…³è”å‡½æ•°ç›´æ–¹å›¾: h_correlation_{group_label}")
        print(f"   - Î”Î·æŠ•å½±: h_eta_projection_{group_label}")
        print(f"   - Î”Ï†æŠ•å½±: h_phi_projection_{group_label}")
        print(f"   - å…ƒæ•°æ®: h_metadata_{group_label}")
        print(f"   - åŸå§‹æ•°æ®æ ‘: correlation_data_{group_label}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜ROOTæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

def process_all_multiplicity_groups():
    """å¤„ç†æ‰€æœ‰5ä¸ªå¤šé‡åº¦åŒºé—´çš„æ–‡ä»¶"""
    print("="*80)
    print("ğŸš€ æ‰¹é‡å¤„ç†æ‰€æœ‰å¤šé‡åº¦åŒºé—´æ–‡ä»¶")
    print("="*80)
    
    # è·å–ç”¨æˆ·è¾“å…¥çš„pTèŒƒå›´
    print("ğŸ“ è¯·è¾“å…¥pTèŒƒå›´...")
    while True:
        try:
            pt_min = float(input("pT_min (GeV): "))
            pt_max = float(input("pT_max (GeV): "))
            if pt_min >= pt_max:
                print("âŒ pTæœ€å°å€¼å¿…é¡»å°äºpTæœ€å¤§å€¼ï¼è¯·é‡æ–°è¾“å…¥")
                continue
            if pt_min < 0 or pt_max < 0:
                print("âŒ pTå€¼å¿…é¡»ä¸ºæ­£æ•°ï¼è¯·é‡æ–°è¾“å…¥")
                continue
            break
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—ï¼è¯·é‡æ–°è¾“å…¥")
    
    print(f"âœ… ç”¨æˆ·è®¾ç½®çš„pTèŒƒå›´: [{pt_min}, {pt_max}] GeV")
    
    # å¤„ç†æ‰€æœ‰5ä¸ªå¤šé‡åº¦åŒºé—´çš„æ–‡ä»¶
    multiplicity_files = [
        "ff+0_20.txt",
        "ff+20_40.txt", 
        "ff+40_60.txt",
        "ff+60_80.txt",
        "ff+80_100.txt"
    ]
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    existing_files = []
    for file_path in multiplicity_files:
        if os.path.exists(file_path):
            existing_files.append(file_path)
            print(f"âœ… æ‰¾åˆ°æ–‡ä»¶: {file_path}")
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    if not existing_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¤šé‡åº¦åˆ†ç»„æ–‡ä»¶ï¼")
        print("è¯·å…ˆè¿è¡Œ extract_high_multiplicity.py ç”Ÿæˆå¤šé‡åº¦åˆ†ç»„æ–‡ä»¶")
        return
    
    print(f"\nğŸ“Š å°†å¤„ç† {len(existing_files)} ä¸ªå¤šé‡åº¦åŒºé—´æ–‡ä»¶")
    
    # ä½¿ç”¨ç”¨æˆ·è¾“å…¥çš„pTèŒƒå›´
    eta_min = -1.1
    eta_max = 1.1
    max_pairs = 10000000
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è®¡ç®—ç»“æœ
    existing_results = check_existing_results(pt_min, pt_max, eta_min, eta_max)
    if existing_results:
        print(f"\nğŸ“‚ å‘ç°å·²å­˜åœ¨çš„ç»“æœæ–‡ä»¶:")
        for result_file in existing_results:
            print(f"   {result_file}")
        
        use_existing = input("\nâ“ æ˜¯å¦ä½¿ç”¨å·²å­˜åœ¨çš„ç»“æœæ–‡ä»¶ï¼Ÿ(y/n): ").lower().strip()
        if use_existing == 'y':
            print("ğŸ“‚ ä½¿ç”¨å·²å­˜åœ¨çš„ç»“æœæ–‡ä»¶è¿›è¡Œç»˜å›¾...")
            # åŠ è½½ç¬¬ä¸€ä¸ªå¯ç”¨çš„ç»“æœæ–‡ä»¶
            results_data = load_results_from_file(existing_results[0])
            if results_data:
                # ç›´æ¥è¿›è¡Œç»˜å›¾
                for group_label in results_data['group_labels']:
                    print(f"\nğŸ“Š ä¸ºå¤šé‡åº¦ç»„ {group_label} ç»˜åˆ¶å›¾è¡¨...")
                    
                    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
                    prefix = f"multiplicity_{group_label}_pt{pt_min}-{pt_max}_eta-{eta_min}-{eta_max}_deltaEta2.2_bins22"
                    output_3d = f"{prefix}_3D.png"
                    output_2d = f"{prefix}_2D.png"
                    output_signal = f"signal_{prefix}.png"
                    output_background = f"background_{prefix}.png"
                    
                    # è·å–è¯¥ç»„çš„æ•°æ®
                    group_data = results_data['group_data'][group_label]
                    eta, phi, C, S_N, B_N = group_data['eta'], group_data['phi'], group_data['C'], group_data['S_N'], group_data['B_N']
                    
                    # ç»˜åˆ¶å›¾è¡¨
                    plot_signal_histogram(eta, phi, S_N, pt_min, pt_max, eta_min, eta_max, output_signal)
                    plot_background_histogram(eta, phi, B_N, pt_min, pt_max, eta_min, eta_max, output_background)
                    plot_3d_correlation_enhanced(eta, phi, C, pt_min, pt_max, eta_min, eta_max, output_3d)
                    plot_2d_correlation_enhanced(eta, phi, C, pt_min, pt_max, eta_min, eta_max, output_2d)
                    
                    print(f"âœ… {group_label} å›¾è¡¨ç»˜åˆ¶å®Œæˆ")
                
                total_elapsed = time.time() - total_start_time
                print(f"\n{'='*60}")
                print("âœ… ä½¿ç”¨å·²å­˜åœ¨ç»“æœå®Œæˆç»˜å›¾ï¼")
                print(f"{'='*60}")
                print(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {total_elapsed/60:.1f} åˆ†é’Ÿ")
                return
    
    print("\nğŸ”„ å¼€å§‹è®¡ç®—æ–°çš„ç»“æœ...")
    total_start_time = time.time()
    
    # å­˜å‚¨æ‰€æœ‰ç»„çš„ç»“æœ
    all_results = {
        'pt_min': pt_min,
        'pt_max': pt_max,
        'eta_min': eta_min,
        'eta_max': eta_max,
        'group_labels': [],
        'group_data': {}
    }
    
    for i, data_file in enumerate(existing_files):
        print(f"\n{'='*60}")
        print(f"ğŸ“ å¤„ç†æ–‡ä»¶ {i+1}/{len(existing_files)}: {os.path.basename(data_file)}")
        print(f"{'='*60}")
        
        try:
            # åŠ è½½æ•°æ®
            event_data = load_data_from_txt(data_file, pt_min, pt_max, eta_min, eta_max)
            if not event_data:
                print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆäº‹ä»¶ï¼Œè·³è¿‡æ­¤æ–‡ä»¶")
                continue
            
            # åˆ†æå¤šé‡åº¦åˆ†å¸ƒ
            analyze_multiplicity_distribution(event_data)
            
            # è®¡ç®—å…³è”å‡½æ•°å’Œç›´æ–¹å›¾
            eta, phi, C, S_N, B_N = calculate_correlation_with_histograms(
                event_data, eta_bins=22, phi_bins=22, max_pairs=max_pairs
            )
            
            if eta is not None:
                # ä»æ–‡ä»¶åæå–å¤šé‡åº¦åŒºé—´æ ‡ç­¾
                base = os.path.basename(data_file)
                group_label = base.replace(".txt", "")
                
                # å­˜å‚¨ç»“æœ
                all_results['group_labels'].append(group_label)
                all_results['group_data'][group_label] = {
                    'eta': eta,
                    'phi': phi,
                    'C': C,
                    'S_N': S_N,
                    'B_N': B_N
                }
                
                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å - ä½¿ç”¨å˜é‡PtèŒƒå›´
                prefix = f"multiplicity_{group_label}_pt{pt_min}-{pt_max}_eta-{eta_min}-{eta_max}_deltaEta2.2_bins22"
                output_3d = f"{prefix}_3D.png"
                output_2d = f"{prefix}_2D.png"
                output_signal = f"signal_{prefix}.png"
                output_background = f"{prefix}.png"
                output_root = f"{prefix}.root"
                
                # ç»˜åˆ¶ä¿¡å·å’ŒèƒŒæ™¯ç›´æ–¹å›¾
                plot_signal_histogram(eta, phi, S_N, pt_min, pt_max, eta_min, eta_max, output_signal)
                plot_background_histogram(eta, phi, B_N, pt_min, pt_max, eta_min, eta_max, output_background)
                
                # ç»˜åˆ¶å…³è”å‡½æ•°
                plot_3d_correlation_enhanced(eta, phi, C, pt_min, pt_max, eta_min, eta_max, output_3d)
                plot_2d_correlation_enhanced(eta, phi, C, pt_min, pt_max, eta_min, eta_max, output_2d)
                
                # ä¿å­˜ROOTæ–‡ä»¶
                save_to_root_file(eta, phi, C, S_N, B_N, pt_min, pt_max, eta_min, eta_max, group_label, output_root)
                
                print(f"âœ… æ–‡ä»¶ {group_label} å¤„ç†å®Œæˆ")
                print(f"ğŸ“Š ç”Ÿæˆå›¾ç‰‡: {output_3d}, {output_2d}, {output_signal}, {output_background}")
                print(f"ğŸ’¾ ç”ŸæˆROOTæ–‡ä»¶: {output_root}")
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶ {data_file} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # ä¿å­˜æ‰€æœ‰ç»“æœåˆ°æ–‡ä»¶
    if all_results['group_labels']:
        print(f"\nğŸ’¾ ä¿å­˜è®¡ç®—ç»“æœåˆ°æ–‡ä»¶...")
        
        # ä¿å­˜ä¸ºpickleæ ¼å¼ï¼ˆæ¨èï¼Œä¿æŒæ•°æ®ç±»å‹ï¼‰
        pkl_file = f"results_pt{pt_min}-{pt_max}_eta{eta_min}-{eta_max}.pkl"
        save_results_to_file(all_results, pkl_file)
        
        # ä¿å­˜ä¸ºjsonæ ¼å¼ï¼ˆå¯è¯»æ€§å¥½ï¼‰
        json_file = f"results_pt{pt_min}-{pt_max}_eta{eta_min}-{eta_max}.json"
        save_results_to_file(all_results, json_file)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜ï¼Œä¸‹æ¬¡è¿è¡Œå¯ç›´æ¥ä½¿ç”¨è¿™äº›æ–‡ä»¶è¿›è¡Œç»˜å›¾ï¼")
    
    total_elapsed = time.time() - total_start_time
    print(f"\n{'='*60}")
    print("âœ… æ‰€æœ‰å¤šé‡åº¦åŒºé—´æ–‡ä»¶å¤„ç†å®Œæˆï¼")
    print(f"{'='*60}")
    print(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {total_elapsed/60:.1f} åˆ†é’Ÿ")
    print(f"ğŸ“Š å¤„ç†äº† {len(existing_files)} ä¸ªæ–‡ä»¶")
    if ROOT_AVAILABLE:
        print(f"ğŸ’¾ ç”Ÿæˆäº†ROOTæ–‡ä»¶ï¼Œå¯åœ¨ROOTä¸­è¿›ä¸€æ­¥åˆ†æ")

def main():
    # å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="Enhanced particle correlation analysis")
    parser.add_argument(
        "--data",
        nargs="+",
        default=["pdata/HM_0_20.txt"],
        help="One or more data files to process"
    )
    parser.add_argument(
        "--format",
        choices=["auto", "txt", "csv"],
        default="auto",
        help="Input file format: auto-detect, txt (space-separated), or csv (comma-separated)"
    )
    parser.add_argument(
        "--all-multiplicity",
        action="store_true",
        help="Process all multiplicity group files automatically"
    )
    parser.add_argument(
        "--plot-only",
        action="store_true",
        help="Only plot graphs using existing results, skip calculation"
    )
    args = parser.parse_args()

    # å¦‚æœæŒ‡å®šäº†--all-multiplicityï¼Œåˆ™æ‰¹é‡å¤„ç†æ‰€æœ‰å¤šé‡åº¦æ–‡ä»¶
    if args.all_multiplicity:
        if args.plot_only:
            # åªç»˜å›¾æ¨¡å¼
            plot_only_mode()
        else:
            # æ­£å¸¸è®¡ç®—æ¨¡å¼
            process_all_multiplicity_groups()
        return

    # è·å–ç”¨æˆ·è¾“å…¥çš„pTèŒƒå›´
    print("ğŸ“ è¯·è¾“å…¥pTèŒƒå›´...")
    while True:
        try:
            pt_min = float(input("pT_min (GeV): "))
            pt_max = float(input("pT_max (GeV): "))
            if pt_min >= pt_max:
                print("âŒ pTæœ€å°å€¼å¿…é¡»å°äºpTæœ€å¤§å€¼ï¼è¯·é‡æ–°è¾“å…¥")
                continue
            if pt_min < 0 or pt_max < 0:
                print("âŒ pTå€¼å¿…é¡»ä¸ºæ­£æ•°ï¼è¯·é‡æ–°è¾“å…¥")
                continue
            break
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—ï¼è¯·é‡æ–°è¾“å…¥")
    
    print(f"âœ… ç”¨æˆ·è®¾ç½®çš„pTèŒƒå›´: [{pt_min}, {pt_max}] GeV")
    
    # ä½¿ç”¨ç”¨æˆ·è¾“å…¥çš„pTèŒƒå›´
    eta_min = -1.1
    eta_max = 1.1
    max_pairs = 10000000

    if MEMORY_MONITOR:
        print("ğŸ“Š Memory monitoring enabled")

    for data_file in args.data:
        if not os.path.exists(data_file):
            print(f"âŒ Error: File not found - {data_file}")
            continue

        print("="*80)
        print("ğŸš€ ENHANCED PARTICLE CORRELATION ANALYSIS WITH HISTOGRAMS")
        print("="*80)
        print(f"ğŸ“ File: {data_file}")
        print(f"ğŸ¯ pT range: [{pt_min}, {pt_max}] GeV")
        print(f"ğŸ¯ |Î·| range: [{eta_min}, {eta_max}]")
        print(f"ğŸ”¢ Max pairs: {max_pairs:,}")

        total_start_time = time.time()

        try:
            # ç¬¬ä¸€é˜¶æ®µï¼šåŠ è½½æ•°æ®ï¼ˆæ ¼å¼è‡ªé€‚åº”/æŒ‡å®šï¼‰
            input_format = args.format
            if input_format == "auto":
                # ç®€å•è‡ªåŠ¨åˆ¤æ–­ï¼šè‹¥æ–‡ä»¶åä»¥ .csv ç»“å°¾æˆ–æ–‡ä»¶å‰å‡ è¡ŒåŒ…å«é€—å·ï¼Œåˆ™è®¤ä¸º csv
                is_csv = data_file.lower().endswith('.csv')
                if not is_csv:
                    with open(data_file, 'r') as _f:
                        for _ in range(10):
                            l = _f.readline()
                            if not l:
                                break
                            if ',' in l:
                                is_csv = True
                                break
                input_format = 'csv' if is_csv else 'txt'

            event_data = load_data_from_txt(data_file, pt_min, pt_max, eta_min, eta_max)
            if not event_data:
                print("âš ï¸ No valid events found!")
                continue

            # åˆ†æå¤šé‡åº¦åˆ†å¸ƒ
            analyze_multiplicity_distribution(event_data)

            # ç¬¬äºŒé˜¶æ®µï¼šè®¡ç®—å…³è”å‡½æ•°å’Œç›´æ–¹å›¾
            eta, phi, C, S_N, B_N = calculate_correlation_with_histograms(
                event_data, eta_bins=22, phi_bins=22, max_pairs=max_pairs
            )

            if eta is not None:
                # æ¨æ–­è¾“å‡ºå‰ç¼€ä¸­çš„å¤šé‡åº¦åŒºé—´
                base = os.path.basename(data_file)
                group_label = base.replace(".txt", "")

                # æ›´æ–°è¾“å‡ºæ–‡ä»¶åå‰ç¼€ - ä½¿ç”¨å˜é‡PtèŒƒå›´
                prefix = f"multiplicity_{group_label}_pt{pt_min}-{pt_max}_eta-{eta_min}-{eta_max}_deltaEta2.2_bins22"
                output_3d = f"{prefix}_3D.png"
                output_2d = f"{prefix}_2D.png"
                output_signal = f"signal_{prefix}.png"
                output_background = f"background_{prefix}.png"
                output_root = f"{prefix}.root"

                # ç»˜åˆ¶ä¿¡å·å’ŒèƒŒæ™¯ç›´æ–¹å›¾
                plot_signal_histogram(eta, phi, S_N, pt_min, pt_max, eta_min, eta_max, output_signal)
                plot_background_histogram(eta, phi, B_N, pt_min, pt_max, eta_min, eta_max, output_background)

                # ç»˜åˆ¶å…³è”å‡½æ•°
                plot_3d_correlation_enhanced(eta, phi, C, pt_min, pt_max, eta_min, eta_max, output_3d)
                plot_2d_correlation_enhanced(eta, phi, C, pt_min, pt_max, eta_min, eta_max, output_2d)
                
                # ä¿å­˜ROOTæ–‡ä»¶
                save_to_root_file(eta, phi, C, S_N, B_N, pt_min, pt_max, eta_min, eta_max, group_label, output_root)

            total_elapsed = time.time() - total_start_time
            print("\n" + "="*60)
            print("âœ… ENHANCED ANALYSIS COMPLETED SUCCESSFULLY")
            print("="*60)
            print(f"â±ï¸ Total processing time: {total_elapsed/60:.1f} minutes")
            print(f"ğŸ¯ Processed {len(event_data):,} events")
            print(f"ğŸ“Š Generated signal and background histograms")
            if ROOT_AVAILABLE:
                print(f"ğŸ’¾ Generated ROOT file for further analysis")

        except Exception as e:
            print(f"âŒ Error during processing: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œè‡ªåŠ¨å¤„ç†æ‰€æœ‰å¤šé‡åº¦æ–‡ä»¶
    import sys
    if len(sys.argv) == 1:
        print("ğŸš€ è‡ªåŠ¨å¤„ç†æ‰€æœ‰å¤šé‡åº¦åŒºé—´æ–‡ä»¶...")
        process_all_multiplicity_groups()
    else:
        main()